{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T19:12:17.723617Z",
     "start_time": "2025-05-24T19:12:10.486065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 1: Setup and Imports\n",
    "\"\"\"\n",
    "Phase 1 Validation Notebook\n",
    "Test and validate all Phase 1 components\n",
    "\"\"\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from config.spark_config import create_spark_session\n",
    "\n",
    "# Cell 2: Initialize Spark\n",
    "spark = create_spark_session(\"Phase1_Validation\")\n",
    "print(f\"Spark UI: http://localhost:4040\")\n",
    "\n",
    "# Cell 3: Load Processed Data\n",
    "df = spark.read.parquet(\"../data/processed/pipeline_features\")\n",
    "print(f\"Total records: {df.count()}\")\n",
    "df.printSchema()\n",
    "\n",
    "# Cell 4: Sentiment Distribution\n",
    "sentiment_dist = df.groupBy(\"sentiment\").count().toPandas()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sentiment_dist['count'], labels=['Negative', 'Positive'], autopct='%1.1f%%')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Cell 5: Feature Analysis\n",
    "feature_stats = df.select(\n",
    "    \"text_length\", \"token_count\", \"vader_compound\",\n",
    "    \"emoji_sentiment\", \"exclamation_count\"\n",
    ").describe().toPandas()\n",
    "display(feature_stats)\n",
    "\n",
    "# Cell 6: VADER vs Actual Sentiment\n",
    "vader_analysis = df.select(\"sentiment\", \"vader_compound\").toPandas()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='sentiment', y='vader_compound', data=vader_analysis)\n",
    "plt.title('VADER Compound Score by Actual Sentiment')\n",
    "plt.show()\n",
    "\n",
    "# Cell 7: Text Length Distribution\n",
    "length_data = df.select(\"sentiment\", \"text_length\").toPandas()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=length_data, x='text_length', hue='sentiment', bins=50)\n",
    "plt.title('Text Length Distribution by Sentiment')\n",
    "plt.show()\n",
    "\n",
    "# Cell 8: Sample Tweets Analysis\n",
    "print(\"Sample positive tweets with high VADER scores:\")\n",
    "df.filter((df.sentiment == 1) & (df.vader_compound > 0.8)) \\\n",
    "  .select(\"text\", \"vader_compound\", \"emoji_sentiment\") \\\n",
    "  .show(5, truncate=False)\n",
    "\n",
    "print(\"\\nSample negative tweets with low VADER scores:\")\n",
    "df.filter((df.sentiment == 0) & (df.vader_compound < -0.8)) \\\n",
    "  .select(\"text\", \"vader_compound\", \"emoji_sentiment\") \\\n",
    "  .show(5, truncate=False)\n",
    "\n",
    "# Cell 9: Feature Correlations\n",
    "numeric_features = [\n",
    "    \"text_length\", \"token_count\", \"emoji_sentiment\",\n",
    "    \"exclamation_count\", \"question_count\", \"uppercase_ratio\",\n",
    "    \"vader_compound\", \"vader_positive\", \"vader_negative\"\n",
    "]\n",
    "corr_data = df.select(numeric_features).toPandas().corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Cell 10: Performance Metrics\n",
    "print(\"Data Processing Performance:\")\n",
    "print(f\"- Total partitions: {df.rdd.getNumPartitions()}\")\n",
    "print(f\"- Spark UI for detailed metrics: http://localhost:4040\")"
   ],
   "id": "849f96ba31cf5e60",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:config.spark_config:Creating Spark session with driver memory: 12g\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/05/24 14:12:12 WARN Utils: Your hostname, Alis-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.8.177 instead (on interface en0)\n",
      "25/05/24 14:12:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/24 14:12:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/24 14:12:13 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "INFO:config.spark_config:Spark session created successfully\n",
      "INFO:config.spark_config:Spark version: 4.0.0\n",
      "INFO:config.spark_config:Driver memory: 12g\n",
      "INFO:config.spark_config:Executor memory: 8g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI: http://localhost:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 14:12:17 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: ../data/processed/pipeline_features.\n",
      "java.io.FileNotFoundException: File ../data/processed/pipeline_features does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/ali/Documents/Projects/micap/data/processed/pipeline_features. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAnalysisException\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSpark UI: http://localhost:4040\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# Cell 3: Load Processed Data\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m df = \u001B[43mspark\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m../data/processed/pipeline_features\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTotal records: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdf.count()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     22\u001B[39m df.printSchema()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/micap/.venv2/lib/python3.11/site-packages/pyspark/sql/readwriter.py:642\u001B[39m, in \u001B[36mDataFrameReader.parquet\u001B[39m\u001B[34m(self, *paths, **options)\u001B[39m\n\u001B[32m    631\u001B[39m int96RebaseMode = options.get(\u001B[33m\"\u001B[39m\u001B[33mint96RebaseMode\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    632\u001B[39m \u001B[38;5;28mself\u001B[39m._set_opts(\n\u001B[32m    633\u001B[39m     mergeSchema=mergeSchema,\n\u001B[32m    634\u001B[39m     pathGlobFilter=pathGlobFilter,\n\u001B[32m   (...)\u001B[39m\u001B[32m    639\u001B[39m     int96RebaseMode=int96RebaseMode,\n\u001B[32m    640\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m642\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jreader\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_spark\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/micap/.venv2/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1356\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1357\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1358\u001B[39m     args_command +\\\n\u001B[32m   1359\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1361\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/micap/.venv2/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:288\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    284\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    285\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    286\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    287\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m288\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    289\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    290\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mAnalysisException\u001B[39m: [PATH_NOT_FOUND] Path does not exist: file:/Users/ali/Documents/Projects/micap/data/processed/pipeline_features. SQLSTATE: 42K03"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
