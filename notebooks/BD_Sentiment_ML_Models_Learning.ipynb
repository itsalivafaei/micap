{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHMu6EWGbv44",
        "outputId": "0bd8ae27-c9f5-4abe-d8fe-e4c6919b0891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow>=2.13.0 transformers matplotlib seaborn scikit-learn\n",
        "\n",
        "# Connect to my google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# copy files from google drive to project\n",
        "!cp '/content/drive/MyDrive/Uni/BD/part-00000.snappy.parquet' '/content/'\n",
        "!cp '/content/drive/MyDrive/Uni/BD/part-00001.snappy.parquet' '/content/'\n",
        "!cp '/content/drive/MyDrive/Uni/BD/part-00002.snappy.parquet' '/content/'\n",
        "!cp '/content/drive/MyDrive/Uni/BD/part-00003.snappy.parquet' '/content/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY9cpuzI48_i"
      },
      "source": [
        "### Now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n2XPjFl5eG4y",
        "outputId": "990316bd-ad70-4221-ee07-df593ee9a2cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:PySpark not available. Will use pandas for data loading.\n",
            "WARNING:__main__:No accelerator detected, using CPU\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7363 - auc: 0.0000e+00 - loss: 0.5106\n",
            "Epoch 1: val_loss improved from inf to 0.05236, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2s/step - accuracy: 0.7414 - auc: 0.0000e+00 - loss: 0.5045 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0524 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9992 - auc: 0.0000e+00 - loss: 0.0520\n",
            "Epoch 2: val_loss improved from 0.05236 to 0.04999, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.9992 - auc: 0.0000e+00 - loss: 0.0515 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0500 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0138\n",
            "Epoch 3: val_loss improved from 0.04999 to 0.03539, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0137 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0354 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0071\n",
            "Epoch 4: val_loss improved from 0.03539 to 0.01963, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0071 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0196 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0044\n",
            "Epoch 5: val_loss improved from 0.01963 to 0.01015, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0044 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0102 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 5.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:__main__:No accelerator detected, using CPU\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8963 - auc: 0.5173 - loss: 0.3879\n",
            "Epoch 1: val_loss improved from inf to 0.21792, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2s/step - accuracy: 0.8979 - auc: 0.5180 - loss: 0.3839 - val_accuracy: 0.9701 - val_auc: 0.4735 - val_loss: 0.2179 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9695 - auc: 0.6665 - loss: 0.1387\n",
            "Epoch 2: val_loss improved from 0.21792 to 0.19818, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.9695 - auc: 0.6682 - loss: 0.1384 - val_accuracy: 0.9701 - val_auc: 0.5352 - val_loss: 0.1982 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9701 - auc: 0.8127 - loss: 0.1136\n",
            "Epoch 3: val_loss improved from 0.19818 to 0.17331, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.9701 - auc: 0.8132 - loss: 0.1135 - val_accuracy: 0.9701 - val_auc: 0.6190 - val_loss: 0.1733 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9715 - auc: 0.8696 - loss: 0.0996\n",
            "Epoch 4: val_loss improved from 0.17331 to 0.15166, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.9715 - auc: 0.8699 - loss: 0.0996 - val_accuracy: 0.9705 - val_auc: 0.7047 - val_loss: 0.1517 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9723 - auc: 0.8978 - loss: 0.0914\n",
            "Epoch 5: val_loss improved from 0.15166 to 0.13632, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.9724 - auc: 0.8979 - loss: 0.0913 - val_accuracy: 0.9713 - val_auc: 0.7863 - val_loss: 0.1363 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 5.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:__main__:No accelerator detected, using CPU\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8850 - auc: 0.0000e+00 - loss: 0.4036\n",
            "Epoch 1: val_loss improved from inf to 0.04410, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.8875 - auc: 0.0000e+00 - loss: 0.3980 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0441 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9995 - auc: 0.0000e+00 - loss: 0.0245\n",
            "Epoch 2: val_loss did not improve from 0.04410\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.9995 - auc: 0.0000e+00 - loss: 0.0242 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0603 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0055\n",
            "Epoch 3: val_loss improved from 0.04410 to 0.04393, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0055 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0439 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 1.0000 - auc: 0.0000e+00 - loss: 0.0027\n",
            "Epoch 4: val_loss improved from 0.04393 to 0.02511, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 1.0000 - auc: 0.0000e+00 - loss: 0.0027 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0251 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 1.0000 - auc: 0.0000e+00 - loss: 0.0017\n",
            "Epoch 5: val_loss improved from 0.02511 to 0.01308, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 1.0000 - auc: 0.0000e+00 - loss: 0.0017 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0131 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 5.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:__main__:No accelerator detected, using CPU\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6789 - auc: 0.0000e+00 - loss: 0.5618\n",
            "Epoch 1: val_loss improved from inf to 0.17322, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.6851 - auc: 0.0000e+00 - loss: 0.5551 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.1732 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9966 - auc: 0.0000e+00 - loss: 0.0992\n",
            "Epoch 2: val_loss improved from 0.17322 to 0.09303, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9967 - auc: 0.0000e+00 - loss: 0.0981 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0930 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9995 - auc: 0.0000e+00 - loss: 0.0233\n",
            "Epoch 3: val_loss improved from 0.09303 to 0.07109, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9995 - auc: 0.0000e+00 - loss: 0.0232 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0711 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0099\n",
            "Epoch 4: val_loss improved from 0.07109 to 0.04289, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9999 - auc: 0.0000e+00 - loss: 0.0099 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0429 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 1.0000 - auc: 0.0000e+00 - loss: 0.0057\n",
            "Epoch 5: val_loss improved from 0.04289 to 0.02277, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 1.0000 - auc: 0.0000e+00 - loss: 0.0057 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0228 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 5.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Deep Learning Models for Sentiment Analysis - GPU/TPU Optimized for Google Colab\n",
        "PIPELINE-COMPATIBLE VERSION - Works with existing MICAP preprocessing pipeline\n",
        "Implements LSTM, CNN, and Transformer models with CUDA/TPU acceleration\n",
        "\n",
        "Author: AI Assistant\n",
        "Date: 2025-01-20\n",
        "Dependencies: tensorflow>=2.13.0, pandas, numpy, pyspark (for data loading)\n",
        "Environment: Google Colab with GPU/TPU runtime\n",
        "Pipeline: Reads from preprocessed parquet files created by MICAP pipeline\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import time\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# TensorFlow imports with GPU optimization\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
        "\n",
        "# Additional ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# PySpark imports for reading existing pipeline data\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql.functions import col\n",
        "    PYSPARK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYSPARK_AVAILABLE = False\n",
        "    logging.warning(\"PySpark not available. Will use pandas for data loading.\")\n",
        "\n",
        "tf.config.optimizer.set_jit(True)      # XLA compilation\n",
        "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
        "\n",
        "# Configure warnings and logging\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class PipelineDataLoader:\n",
        "    \"\"\"\n",
        "    Loads data from MICAP preprocessing pipeline (parquet files)\n",
        "    Maintains compatibility with existing feature engineering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize pipeline data loader.\"\"\"\n",
        "        self.spark = None\n",
        "        if PYSPARK_AVAILABLE:\n",
        "            self._init_spark_session()\n",
        "\n",
        "    def _init_spark_session(self):\n",
        "        \"\"\"Initialize Spark session for reading parquet files.\"\"\"\n",
        "        try:\n",
        "            self.spark = (SparkSession.builder\n",
        "                         .appName(\"ColabDataLoader\")\n",
        "                         .master(\"local[*]\")\n",
        "                         .config(\"spark.driver.memory\", \"64g\")\n",
        "                         .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "                         .getOrCreate())\n",
        "            self.spark.sparkContext.setLogLevel(\"WARN\")\n",
        "            logger.info(\"Spark session created for pipeline data loading\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create Spark session: {e}\")\n",
        "            self.spark = None\n",
        "\n",
        "    def load_pipeline_data(self, data_path: str,\n",
        "                          sample_fraction: float = 1.0,\n",
        "                          use_spark: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load preprocessed data from MICAP pipeline.\n",
        "\n",
        "        Args:\n",
        "            data_path: Path to processed parquet file or directory\n",
        "            sample_fraction: Fraction of data to use\n",
        "            use_spark: Whether to use Spark for loading (fallback to pandas)\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Loaded and sampled data\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading pipeline data from: {data_path}\")\n",
        "\n",
        "        if use_spark and self.spark and PYSPARK_AVAILABLE:\n",
        "            return self._load_with_spark(data_path, sample_fraction)\n",
        "        else:\n",
        "            return self._load_with_pandas(data_path, sample_fraction)\n",
        "\n",
        "    def _load_with_spark(self, data_path: str, sample_fraction: float) -> pd.DataFrame:\n",
        "        \"\"\"Load data using Spark (maintains original pipeline compatibility).\"\"\"\n",
        "        logger.info(\"Loading data with Spark...\")\n",
        "\n",
        "        try:\n",
        "            # Read parquet file(s)\n",
        "            df = self.spark.read.parquet(data_path)\n",
        "\n",
        "            # Sample if requested\n",
        "            if sample_fraction < 1.0:\n",
        "                df = df.sample(sample_fraction, seed=42)\n",
        "\n",
        "            # Convert to pandas for TensorFlow compatibility\n",
        "            # Use efficient streaming for large datasets\n",
        "            pandas_df = self._spark_to_pandas_efficient(df)\n",
        "\n",
        "            logger.info(f\"Loaded {len(pandas_df)} records with Spark\")\n",
        "            return pandas_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Spark loading failed: {e}\")\n",
        "            logger.info(\"Falling back to pandas...\")\n",
        "            return self._load_with_pandas(data_path, sample_fraction)\n",
        "\n",
        "    def _load_with_pandas(self, data_path: str, sample_fraction: float) -> pd.DataFrame:\n",
        "        \"\"\"Load data with pandas (fallback method).\"\"\"\n",
        "        logger.info(\"Loading data with pandas...\")\n",
        "\n",
        "        try:\n",
        "            # Try reading as parquet first\n",
        "            if data_path.endswith('.parquet') or os.path.isdir(data_path):\n",
        "                df = pd.read_parquet(data_path)\n",
        "            else:\n",
        "                # Fallback to CSV\n",
        "                df = pd.read_csv(data_path)\n",
        "\n",
        "            # Sample if requested\n",
        "            if sample_fraction < 1.0:\n",
        "                df = df.sample(frac=sample_fraction, random_state=42)\n",
        "\n",
        "            logger.info(f\"Loaded {len(df)} records with pandas\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _spark_to_pandas_efficient(self, spark_df, batch_size: int = 50000) -> pd.DataFrame:\n",
        "        \"\"\"Efficiently convert Spark DataFrame to pandas using streaming.\"\"\"\n",
        "        try:\n",
        "            # Try direct conversion for smaller datasets\n",
        "            if spark_df.count() < batch_size:\n",
        "                return spark_df.toPandas()\n",
        "\n",
        "            # Stream large datasets in batches\n",
        "            logger.info(\"Streaming large dataset in batches...\")\n",
        "            parts = []\n",
        "            for batch in spark_df.toLocalIterator(batch_size):\n",
        "                batch_df = pd.DataFrame(list(batch), columns=spark_df.columns)\n",
        "                parts.append(batch_df)\n",
        "\n",
        "            return pd.concat(parts, ignore_index=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Streaming failed, using direct conversion: {e}\")\n",
        "            return spark_df.toPandas()\n",
        "\n",
        "    def validate_pipeline_features(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"\n",
        "        Validate that the DataFrame contains expected pipeline features.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to validate\n",
        "\n",
        "        Returns:\n",
        "            bool: True if valid pipeline data\n",
        "        \"\"\"\n",
        "        # Expected features from MICAP pipeline\n",
        "        required_features = [\n",
        "            'text', 'sentiment', 'text_processed',\n",
        "            'text_length', 'processed_length', 'token_count'\n",
        "        ]\n",
        "\n",
        "        # Optional but expected features\n",
        "        expected_features = [\n",
        "            'vader_compound', 'vader_positive', 'vader_negative', 'vader_neutral',\n",
        "            'emoji_sentiment', 'exclamation_count', 'question_count',\n",
        "            'uppercase_ratio', 'punctuation_density',\n",
        "            'hour_sin', 'hour_cos', 'is_weekend'\n",
        "        ]\n",
        "\n",
        "        # Check required features\n",
        "        missing_required = [f for f in required_features if f not in df.columns]\n",
        "        if missing_required:\n",
        "            logger.error(f\"Missing required pipeline features: {missing_required}\")\n",
        "            return False\n",
        "\n",
        "        # Log available optional features\n",
        "        available_optional = [f for f in expected_features if f in df.columns]\n",
        "        logger.info(f\"Available pipeline features: {len(available_optional)}/{len(expected_features)}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def prepare_features_for_training(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
        "        \"\"\"\n",
        "        Prepare features from pipeline data for deep learning training.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with pipeline features\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (text_sequences, labels, feature_info)\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing pipeline features for training...\")\n",
        "\n",
        "        # Validate data\n",
        "        if not self.validate_pipeline_features(df):\n",
        "            raise ValueError(\"Invalid pipeline data structure\")\n",
        "\n",
        "        # Use processed text for deep learning (already cleaned by pipeline)\n",
        "        text_column = 'text_processed' if 'text_processed' in df.columns else 'text'\n",
        "        texts = df[text_column].fillna('').astype(str)\n",
        "        labels = df['sentiment'].values\n",
        "\n",
        "        # Extract numeric features created by pipeline\n",
        "        numeric_features = []\n",
        "        feature_names = []\n",
        "\n",
        "        # Basic text features\n",
        "        if 'text_length' in df.columns:\n",
        "            numeric_features.append(df['text_length'].fillna(0))\n",
        "            feature_names.append('text_length')\n",
        "\n",
        "        if 'token_count' in df.columns:\n",
        "            numeric_features.append(df['token_count'].fillna(0))\n",
        "            feature_names.append('token_count')\n",
        "\n",
        "        # VADER sentiment features\n",
        "        vader_features = ['vader_compound', 'vader_positive', 'vader_negative', 'vader_neutral']\n",
        "        for feature in vader_features:\n",
        "            if feature in df.columns:\n",
        "                numeric_features.append(df[feature].fillna(0))\n",
        "                feature_names.append(feature)\n",
        "\n",
        "        # Emoji and text statistics\n",
        "        text_stat_features = ['emoji_sentiment', 'exclamation_count', 'question_count',\n",
        "                             'uppercase_ratio', 'punctuation_density']\n",
        "        for feature in text_stat_features:\n",
        "            if feature in df.columns:\n",
        "                numeric_features.append(df[feature].fillna(0))\n",
        "                feature_names.append(feature)\n",
        "\n",
        "        # Temporal features\n",
        "        temporal_features = ['hour_sin', 'hour_cos', 'is_weekend']\n",
        "        for feature in temporal_features:\n",
        "            if feature in df.columns:\n",
        "                numeric_features.append(df[feature].fillna(0))\n",
        "                feature_names.append(feature)\n",
        "\n",
        "        # Combine numeric features\n",
        "        if numeric_features:\n",
        "            numeric_array = np.column_stack(numeric_features)\n",
        "            logger.info(f\"Extracted {len(feature_names)} numeric features: {feature_names}\")\n",
        "        else:\n",
        "            numeric_array = None\n",
        "            logger.warning(\"No numeric features found in pipeline data\")\n",
        "\n",
        "        feature_info = {\n",
        "            'text_column': text_column,\n",
        "            'numeric_features': feature_names,\n",
        "            'numeric_shape': numeric_array.shape if numeric_array is not None else None,\n",
        "            'text_samples': len(texts),\n",
        "            'label_distribution': pd.Series(labels).value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Feature preparation completed: {feature_info}\")\n",
        "\n",
        "        return texts.values, labels, numeric_array, feature_info\n",
        "\n",
        "class TestDataEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluates trained models on test dataset with TPU optimization\n",
        "    Designed for Google Colab TPU v6e1 with 173GB RAM\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, env_manager: GPUEnvironmentManager, data_loader: PipelineDataLoader):\n",
        "        \"\"\"\n",
        "        Initialize test evaluator with environment and data loader.\n",
        "        \n",
        "        Args:\n",
        "            env_manager: GPU/TPU environment manager\n",
        "            data_loader: Pipeline data loader instance\n",
        "        \"\"\"\n",
        "        self.env_manager = env_manager\n",
        "        self.data_loader = data_loader\n",
        "        self.test_batch_size = self._calculate_optimal_test_batch_size()\n",
        "        \n",
        "    def _calculate_optimal_test_batch_size(self) -> int:\n",
        "        \"\"\"Calculate optimal batch size for TPU v6e1 with 173GB RAM\"\"\"\n",
        "        # TPU v6e1 can handle very large batches\n",
        "        if self.env_manager.device_type == 'TPU':\n",
        "            # Use larger batch size for TPU with 173GB RAM\n",
        "            return 65536  # 64K batch size for maximum throughput\n",
        "        elif self.env_manager.device_type == 'GPU':\n",
        "            return 16384\n",
        "        else:\n",
        "            return 1024\n",
        "            \n",
        "    def load_test_data(self, test_path: str = '/content/drive/MyDrive/data/raw/testdata.manual.2009.06.14.csv') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load test dataset from CSV file.\n",
        "        \n",
        "        Args:\n",
        "            test_path: Path to test CSV file\n",
        "            \n",
        "        Returns:\n",
        "            pd.DataFrame: Loaded test data\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading test data from: {test_path}\")\n",
        "        \n",
        "        try:\n",
        "            # Define column names for Sentiment140 test data\n",
        "            column_names = ['polarity', 'tweet_id', 'date', 'query', 'user', 'text']\n",
        "            \n",
        "            # Load CSV with proper encoding\n",
        "            test_df = pd.read_csv(\n",
        "                test_path,\n",
        "                names=column_names,\n",
        "                encoding='latin-1',\n",
        "                header=None\n",
        "            )\n",
        "            \n",
        "            # Convert polarity (0=negative, 4=positive) to binary sentiment\n",
        "            test_df['sentiment'] = test_df['polarity'].apply(lambda x: 0 if x == 0 else 1)\n",
        "            \n",
        "            # Basic preprocessing to match pipeline format\n",
        "            test_df['text_processed'] = test_df['text'].str.lower().str.strip()\n",
        "            test_df['text_length'] = test_df['text'].str.len()\n",
        "            test_df['processed_length'] = test_df['text_processed'].str.len()\n",
        "            test_df['token_count'] = test_df['text_processed'].str.split().str.len()\n",
        "            \n",
        "            logger.info(f\"Loaded {len(test_df)} test samples\")\n",
        "            logger.info(f\"Test set class distribution:\\n{test_df['sentiment'].value_counts()}\")\n",
        "            \n",
        "            return test_df\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load test data: {e}\")\n",
        "            raise\n",
        "            \n",
        "    def prepare_test_features(self, test_df: pd.DataFrame, tokenizer: Tokenizer, \n",
        "                            max_length: int = 100, numeric_features_dim: int = 0) -> Tuple:\n",
        "        \"\"\"\n",
        "        Prepare test data features matching training format.\n",
        "        \n",
        "        Args:\n",
        "            test_df: Test DataFrame\n",
        "            tokenizer: Trained tokenizer from model\n",
        "            max_length: Maximum sequence length\n",
        "            numeric_features_dim: Number of numeric features\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of prepared test features\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing test features...\")\n",
        "        \n",
        "        # Use processed text\n",
        "        text_column = 'text_processed' if 'text_processed' in test_df.columns else 'text'\n",
        "        texts = test_df[text_column].fillna('').astype(str)\n",
        "        \n",
        "        # Convert to sequences using the trained tokenizer\n",
        "        sequences = tokenizer.texts_to_sequences(texts)\n",
        "        X_text = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "        \n",
        "        # Prepare labels\n",
        "        y_test = test_df['sentiment'].values.astype(np.float32)\n",
        "        \n",
        "        # Prepare numeric features if model uses them\n",
        "        X_numeric = None\n",
        "        if numeric_features_dim > 0:\n",
        "            # Create dummy numeric features for test data\n",
        "            # In production, you'd run the full feature engineering pipeline\n",
        "            numeric_features = []\n",
        "            \n",
        "            # Basic features we can calculate\n",
        "            if 'text_length' in test_df.columns:\n",
        "                numeric_features.append(test_df['text_length'].fillna(0))\n",
        "            if 'token_count' in test_df.columns:\n",
        "                numeric_features.append(test_df['token_count'].fillna(0))\n",
        "                \n",
        "            # Add dummy values for missing features\n",
        "            while len(numeric_features) < numeric_features_dim:\n",
        "                numeric_features.append(np.zeros(len(test_df)))\n",
        "                \n",
        "            X_numeric = np.column_stack(numeric_features[:numeric_features_dim]).astype(np.float32)\n",
        "            logger.info(f\"Created {X_numeric.shape[1]} numeric features for test data\")\n",
        "            \n",
        "        return X_text, X_numeric, y_test\n",
        "        \n",
        "    def evaluate_model(self, model_path: str, test_data: Tuple, model_name: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate a single model on test data with TPU optimization.\n",
        "        \n",
        "        Args:\n",
        "            model_path: Path to saved model\n",
        "            test_data: Prepared test data tuple\n",
        "            model_name: Name of the model\n",
        "            \n",
        "        Returns:\n",
        "            Dict: Evaluation metrics\n",
        "        \"\"\"\n",
        "        logger.info(f\"Evaluating {model_name} on test data...\")\n",
        "        \n",
        "        try:\n",
        "            # Load model with strategy scope\n",
        "            with self.env_manager.strategy.scope():\n",
        "                model = tf.keras.models.load_model(model_path)\n",
        "                \n",
        "            # Prepare test inputs\n",
        "            if len(test_data) == 3:  # Has numeric features\n",
        "                X_text, X_numeric, y_test = test_data\n",
        "                test_inputs = [X_text, X_numeric]\n",
        "            else:\n",
        "                X_text, y_test = test_data\n",
        "                test_inputs = X_text\n",
        "                \n",
        "            # Create optimized dataset for TPU\n",
        "            if self.env_manager.device_type == 'TPU':\n",
        "                test_dataset = self._create_tpu_dataset(test_inputs, y_test)\n",
        "                \n",
        "                # Evaluate with TPU strategy\n",
        "                with self.env_manager.strategy.scope():\n",
        "                    eval_results = model.evaluate(\n",
        "                        test_dataset,\n",
        "                        verbose=1,\n",
        "                        return_dict=True\n",
        "                    )\n",
        "                    \n",
        "                    # Get predictions for detailed metrics\n",
        "                    predictions = model.predict(test_dataset, verbose=1)\n",
        "                    \n",
        "            else:\n",
        "                # Regular evaluation for GPU/CPU\n",
        "                eval_results = model.evaluate(\n",
        "                    test_inputs, y_test,\n",
        "                    batch_size=self.test_batch_size,\n",
        "                    verbose=1,\n",
        "                    return_dict=True\n",
        "                )\n",
        "                \n",
        "                predictions = model.predict(\n",
        "                    test_inputs,\n",
        "                    batch_size=self.test_batch_size,\n",
        "                    verbose=1\n",
        "                )\n",
        "                \n",
        "            # Calculate additional metrics\n",
        "            y_pred = (predictions > 0.5).astype(int).flatten()\n",
        "            \n",
        "            # Classification report\n",
        "            from sklearn.metrics import classification_report, confusion_matrix\n",
        "            class_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "            \n",
        "            # Compile results\n",
        "            results = {\n",
        "                'model_name': model_name,\n",
        "                'test_loss': eval_results.get('loss', None),\n",
        "                'test_accuracy': eval_results.get('accuracy', None),\n",
        "                'test_auc': eval_results.get('auc', None),\n",
        "                'classification_report': class_report,\n",
        "                'confusion_matrix': conf_matrix.tolist(),\n",
        "                'test_samples': len(y_test),\n",
        "                'batch_size_used': self.test_batch_size\n",
        "            }\n",
        "            \n",
        "            # Log summary\n",
        "            logger.info(f\"{model_name} Test Results:\")\n",
        "            logger.info(f\"  Loss: {results['test_loss']:.4f}\")\n",
        "            logger.info(f\"  Accuracy: {results['test_accuracy']:.4f}\")\n",
        "            if results['test_auc']:\n",
        "                logger.info(f\"  AUC: {results['test_auc']:.4f}\")\n",
        "            logger.info(f\"  Precision: {class_report['weighted avg']['precision']:.4f}\")\n",
        "            logger.info(f\"  Recall: {class_report['weighted avg']['recall']:.4f}\")\n",
        "            logger.info(f\"  F1-Score: {class_report['weighted avg']['f1-score']:.4f}\")\n",
        "            \n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to evaluate {model_name}: {e}\")\n",
        "            return {'model_name': model_name, 'error': str(e)}\n",
        "            \n",
        "    def _create_tpu_dataset(self, inputs, labels, shuffle: bool = False):\n",
        "        \"\"\"\n",
        "        Create TPU-optimized dataset with maximum batch size.\n",
        "        \n",
        "        Args:\n",
        "            inputs: Input data (text and/or numeric features)\n",
        "            labels: Target labels\n",
        "            shuffle: Whether to shuffle data\n",
        "            \n",
        "        Returns:\n",
        "            tf.data.Dataset optimized for TPU\n",
        "        \"\"\"\n",
        "        # Create dataset from tensor slices\n",
        "        if isinstance(inputs, list):\n",
        "            dataset = tf.data.Dataset.from_tensor_slices((tuple(inputs), labels))\n",
        "        else:\n",
        "            dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "            \n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=len(labels))\n",
        "            \n",
        "        # Batch with TPU-optimized size and prefetch\n",
        "        dataset = dataset.batch(self.test_batch_size, drop_remainder=False)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        \n",
        "        # Cache in memory for TPU (we have 173GB RAM)\n",
        "        dataset = dataset.cache()\n",
        "        \n",
        "        return dataset\n",
        "        \n",
        "    def visualize_results(self, results: List[Dict], save_path: str = '/content/'):\n",
        "        \"\"\"\n",
        "        Create visualization of test results.\n",
        "        \n",
        "        Args:\n",
        "            results: List of evaluation results\n",
        "            save_path: Path to save visualizations\n",
        "        \"\"\"\n",
        "        logger.info(\"Creating test results visualization...\")\n",
        "        \n",
        "        # Create comparison plot\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        \n",
        "        # Extract metrics for plotting\n",
        "        model_names = [r['model_name'] for r in results if 'error' not in r]\n",
        "        accuracies = [r['test_accuracy'] for r in results if 'error' not in r]\n",
        "        losses = [r['test_loss'] for r in results if 'error' not in r]\n",
        "        f1_scores = [r['classification_report']['weighted avg']['f1-score'] \n",
        "                    for r in results if 'error' not in r]\n",
        "        \n",
        "        # Plot 1: Accuracy comparison\n",
        "        ax = axes[0, 0]\n",
        "        bars = ax.bar(model_names, accuracies, color='skyblue', edgecolor='navy', alpha=0.7)\n",
        "        ax.set_ylabel('Accuracy', fontsize=12)\n",
        "        ax.set_title('Model Accuracy on Test Set', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylim(0, 1)\n",
        "        # Add value labels\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                   f'{acc:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "        \n",
        "        # Plot 2: Loss comparison\n",
        "        ax = axes[0, 1]\n",
        "        bars = ax.bar(model_names, losses, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
        "        ax.set_ylabel('Loss', fontsize=12)\n",
        "        ax.set_title('Model Loss on Test Set', fontsize=14, fontweight='bold')\n",
        "        # Add value labels\n",
        "        for bar, loss in zip(bars, losses):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                   f'{loss:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "        \n",
        "        # Plot 3: F1-Score comparison\n",
        "        ax = axes[1, 0]\n",
        "        bars = ax.bar(model_names, f1_scores, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
        "        ax.set_ylabel('F1-Score', fontsize=12)\n",
        "        ax.set_title('Model F1-Score on Test Set', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylim(0, 1)\n",
        "        # Add value labels\n",
        "        for bar, f1 in zip(bars, f1_scores):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                   f'{f1:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "        \n",
        "        # Plot 4: Confusion Matrix for best model\n",
        "        ax = axes[1, 1]\n",
        "        best_model = max(results, key=lambda x: x.get('test_accuracy', 0) if 'error' not in x else 0)\n",
        "        if 'confusion_matrix' in best_model:\n",
        "            conf_matrix = np.array(best_model['confusion_matrix'])\n",
        "            im = ax.imshow(conf_matrix, cmap='Blues', aspect='auto')\n",
        "            \n",
        "            # Add colorbar\n",
        "            plt.colorbar(im, ax=ax)\n",
        "            \n",
        "            # Add labels\n",
        "            ax.set_xticks([0, 1])\n",
        "            ax.set_yticks([0, 1])\n",
        "            ax.set_xticklabels(['Negative', 'Positive'])\n",
        "            ax.set_yticklabels(['Negative', 'Positive'])\n",
        "            ax.set_xlabel('Predicted', fontsize=12)\n",
        "            ax.set_ylabel('Actual', fontsize=12)\n",
        "            ax.set_title(f'Confusion Matrix - {best_model[\"model_name\"]}', fontsize=14, fontweight='bold')\n",
        "            \n",
        "            # Add text annotations\n",
        "            for i in range(2):\n",
        "                for j in range(2):\n",
        "                    ax.text(j, i, str(conf_matrix[i, j]), \n",
        "                           ha='center', va='center', color='black', fontsize=14)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{save_path}test_results_visualization.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        logger.info(f\"Visualization saved to {save_path}test_results_visualization.png\")\n",
        "\n",
        "class GPUEnvironmentManager:\n",
        "    \"\"\"Manages GPU/TPU environment setup and optimization for Google Colab\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device_type = self._detect_accelerator()\n",
        "        self.strategy = self._setup_distribution_strategy()\n",
        "        self.mixed_precision_enabled = False\n",
        "        # self._configure_mixed_precision()\n",
        "        self._log_environment_info()\n",
        "\n",
        "        # if self.device_type == 'GPU':\n",
        "            # self._setup_mixed_precision()\n",
        "\n",
        "    def _detect_accelerator(self) -> str:\n",
        "        \"\"\"Detect available accelerator (GPU/TPU/CPU)\"\"\"\n",
        "        try:\n",
        "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            logger.info(\"TPU detected and initialized\")\n",
        "            return 'TPU'\n",
        "        except (ValueError, RuntimeError):\n",
        "            pass\n",
        "\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            logger.info(f\"GPU detected: {len(gpus)} device(s)\")\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            return 'GPU'\n",
        "\n",
        "        logger.warning(\"No accelerator detected, using CPU\")\n",
        "        return 'CPU'\n",
        "\n",
        "    def _setup_distribution_strategy(self):\n",
        "        \"\"\"Setup distribution strategy based on hardware\"\"\"\n",
        "        if self.device_type == 'TPU':\n",
        "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "            strategy = tf.distribute.TPUStrategy(tpu)\n",
        "            logger.info(f\"Using TPU strategy with {strategy.num_replicas_in_sync} replicas\")\n",
        "        elif self.device_type == 'GPU':\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "            logger.info(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} replicas\")\n",
        "        else:\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "            logger.info(\"Using default strategy (CPU)\")\n",
        "\n",
        "        return strategy\n",
        "\n",
        "    def _setup_mixed_precision(self):\n",
        "        \"\"\"Setup mixed precision training for faster GPU training\"\"\"\n",
        "        try:\n",
        "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "            tf.keras.mixed_precision.set_global_policy(policy)\n",
        "            self.mixed_precision_enabled = True\n",
        "            logger.info(\"Mixed precision training enabled (float16)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not enable mixed precision: {e}\")\n",
        "\n",
        "    def _log_environment_info(self):\n",
        "        \"\"\"Log environment information\"\"\"\n",
        "        logger.info(\"=== GPU/TPU Environment Information ===\")\n",
        "        logger.info(f\"TensorFlow version: {tf.__version__}\")\n",
        "        logger.info(f\"Detected accelerator: {self.device_type}\")\n",
        "\n",
        "        if self.device_type == 'GPU':\n",
        "            gpus = tf.config.list_physical_devices('GPU')\n",
        "            for i, gpu in enumerate(gpus):\n",
        "                logger.info(f\"GPU {i}: {gpu}\")\n",
        "\n",
        "        logger.info(\"=\" * 50)\n",
        "\n",
        "    def get_optimal_batch_size(self, base_batch_size: int = 16384) -> int:\n",
        "    \"\"\"\n",
        "    Calculate optimal batch size based on hardware - \n",
        "    optimized for v6e1 TPU\n",
        "    \"\"\"\n",
        "    \n",
        "    if self.device_type == 'TPU':\n",
        "        # v6e1 TPU with 173GB RAM can handle massive batches\n",
        "        # Use power of 2 for optimal TPU performance\n",
        "        return 65536  # 64K for training\n",
        "    elif self.device_type == 'GPU':\n",
        "        return base_batch_size * max(1, self.strategy.num_replicas_in_sync)\n",
        "    else:\n",
        "        return max(16, base_batch_size // 2)\n",
        "\n",
        "\n",
        "class PipelineOptimizedModel:\n",
        "    \"\"\"\n",
        "    Base class for pipeline-compatible GPU/TPU optimized deep learning models\n",
        "    Works with features from MICAP preprocessing pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_manager: GPUEnvironmentManager,\n",
        "                 max_words: int = 10000, max_length: int = 100):\n",
        "        self.env_manager = env_manager\n",
        "        self.max_words = max_words\n",
        "        self.max_length = max_length\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.history = None\n",
        "        self.training_time = 0\n",
        "        self.numeric_features_dim = 0\n",
        "\n",
        "    def prepare_data(self, texts: np.ndarray, labels: np.ndarray,\n",
        "                    numeric_features: Optional[np.ndarray] = None,\n",
        "                    validation_split: float = 0.2) -> Tuple:\n",
        "        \"\"\"\n",
        "        Prepare text and numeric features for training.\n",
        "\n",
        "        Args:\n",
        "            texts: Array of text data\n",
        "            labels: Array of labels\n",
        "            numeric_features: Optional array of numeric features from pipeline\n",
        "            validation_split: Validation split ratio\n",
        "\n",
        "        Returns:\n",
        "            Tuple of prepared datasets\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing data for pipeline-compatible training...\")\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        self.tokenizer = Tokenizer(\n",
        "            num_words=self.max_words,\n",
        "            oov_token='<OOV>',\n",
        "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "        )\n",
        "\n",
        "        # Fit tokenizer\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "\n",
        "        # Convert to sequences\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X_text = pad_sequences(sequences, maxlen=self.max_length,\n",
        "                              padding='post', truncating='post')\n",
        "\n",
        "        # Prepare numeric features if available\n",
        "        X_numeric = None\n",
        "        if numeric_features is not None:\n",
        "            X_numeric = numeric_features.astype(np.float32)\n",
        "            self.numeric_features_dim = X_numeric.shape[1]\n",
        "            logger.info(f\"Using {self.numeric_features_dim} numeric features from pipeline\")\n",
        "\n",
        "        y = labels.astype(np.float32)\n",
        "\n",
        "        # Split data\n",
        "        if X_numeric is not None:\n",
        "            X_text_train, X_text_val, X_num_train, X_num_val, y_train, y_val = train_test_split(\n",
        "                X_text, X_numeric, y, test_size=validation_split,\n",
        "                random_state=42, stratify=y\n",
        "            )\n",
        "\n",
        "            return (X_text_train, X_num_train, y_train), (X_text_val, X_num_val, y_val)\n",
        "        else:\n",
        "            X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
        "                X_text, y, test_size=validation_split,\n",
        "                random_state=42, stratify=y\n",
        "            )\n",
        "\n",
        "            return (X_text_train, y_train), (X_text_val, y_val)\n",
        "\n",
        "    def _get_optimizer(self, learning_rate: float = 0.001):\n",
        "        \"\"\"Get optimized optimizer\"\"\"\n",
        "        if self.env_manager.device_type == 'TPU':\n",
        "            optimizer = optimizers.Adam(learning_rate=learning_rate * 2)\n",
        "        else:\n",
        "            optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        if self.env_manager.mixed_precision_enabled:\n",
        "            optimizer = LossScaleOptimizer(optimizer)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def _get_callbacks(self, model_name: str, patience: int = 5):\n",
        "        \"\"\"Get training callbacks\"\"\"\n",
        "        return [\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor='val_loss', patience=patience,\n",
        "                restore_best_weights=True, verbose=1\n",
        "            ),\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss', factor=0.5,\n",
        "                patience=max(2, patience // 2), min_lr=1e-7, verbose=1\n",
        "            ),\n",
        "            callbacks.ModelCheckpoint(\n",
        "                filepath=f'/content/best_{model_name}_pipeline_model.h5',\n",
        "                monitor='val_loss', save_best_only=True, verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "\n",
        "class PipelineLSTMModel(PipelineOptimizedModel):\n",
        "    \"\"\"LSTM model optimized for pipeline features\"\"\"\n",
        "\n",
        "    def build_model(self, embedding_dim: int = 128, lstm_units: int = 64,\n",
        "                   dropout_rate: float = 0.3):\n",
        "        \"\"\"Build LSTM model with optional numeric features integration\"\"\"\n",
        "        logger.info(\"Building pipeline-compatible LSTM model...\")\n",
        "\n",
        "        with self.env_manager.strategy.scope():\n",
        "            # Text input branch\n",
        "            text_input = layers.Input(shape=(self.max_length,), name='text_input')\n",
        "\n",
        "            # Embedding layer\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=self.max_words,\n",
        "                output_dim=embedding_dim,\n",
        "                input_length=self.max_length,\n",
        "                mask_zero=True,\n",
        "                name='embedding'\n",
        "            )(text_input)\n",
        "\n",
        "            embedding = layers.SpatialDropout1D(dropout_rate * 0.5)(embedding)\n",
        "\n",
        "            # Bidirectional LSTM layers\n",
        "            lstm1 = layers.Bidirectional(\n",
        "                layers.LSTM(lstm_units, dropout=dropout_rate,\n",
        "                            return_sequences=True),\n",
        "                name='bi_lstm_1'\n",
        "            )(embedding)\n",
        "\n",
        "            lstm2 = layers.Bidirectional(\n",
        "                layers.LSTM(lstm_units // 2, dropout=dropout_rate,\n",
        "                            return_sequences=False),\n",
        "                name='bi_lstm_2'\n",
        "            )(lstm1)\n",
        "\n",
        "            # Text features\n",
        "            text_features = layers.Dense(64, activation='relu', name='text_dense')(lstm2)\n",
        "            text_features = layers.BatchNormalization()(text_features)\n",
        "            text_features = layers.Dropout(dropout_rate)(text_features)\n",
        "\n",
        "            # Combine with numeric features if available\n",
        "            if self.numeric_features_dim > 0:\n",
        "                # Numeric input branch\n",
        "                numeric_input = layers.Input(shape=(self.numeric_features_dim,), name='numeric_input')\n",
        "                numeric_features = layers.Dense(32, activation='relu', name='numeric_dense')(numeric_input)\n",
        "                numeric_features = layers.BatchNormalization()(numeric_features)\n",
        "                numeric_features = layers.Dropout(dropout_rate * 0.5)(numeric_features)\n",
        "\n",
        "                # Combine text and numeric features\n",
        "                combined = layers.Concatenate(name='combine_features')([text_features, numeric_features])\n",
        "                inputs = [text_input, numeric_input]\n",
        "            else:\n",
        "                combined = text_features\n",
        "                inputs = text_input\n",
        "\n",
        "            # Final classification layers\n",
        "            dense = layers.Dense(32, activation='relu', name='final_dense')(combined)\n",
        "            dense = layers.Dropout(dropout_rate * 0.5)(dense)\n",
        "\n",
        "            output = layers.Dense(1, activation='sigmoid', name='output')(dense)\n",
        "\n",
        "            model = models.Model(inputs=inputs, outputs=output, name='PipelineLSTM')\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        # Compile model\n",
        "        optimizer = self._get_optimizer()\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "        )\n",
        "\n",
        "        logger.info(\"LSTM Model Architecture:\")\n",
        "        self.model.summary(print_fn=logger.info)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, train_data: Tuple, val_data: Tuple, epochs: int = 10):\n",
        "        \"\"\"Train the LSTM model\"\"\"\n",
        "        logger.info(\"Training pipeline LSTM model...\")\n",
        "\n",
        "        callbacks_list = self._get_callbacks('LSTM')\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Prepare training data\n",
        "        if self.numeric_features_dim > 0:\n",
        "            X_text_train, X_num_train, y_train = train_data\n",
        "            X_text_val, X_num_val, y_val = val_data\n",
        "\n",
        "            train_inputs = [X_text_train, X_num_train]\n",
        "            val_inputs = [X_text_val, X_num_val]\n",
        "        else:\n",
        "            X_text_train, y_train = train_data\n",
        "            X_text_val, y_val = val_data\n",
        "\n",
        "            train_inputs = X_text_train\n",
        "            val_inputs = X_text_val\n",
        "\n",
        "        batch_size = self.env_manager.get_optimal_batch_size()\n",
        "\n",
        "        # with self.env_manager.strategy.scope():\n",
        "        #     self.history = self.model.fit(\n",
        "        #         train_inputs, y_train,\n",
        "        #         batch_size=batch_size,\n",
        "        #         epochs=epochs,\n",
        "        #         validation_data=(val_inputs, y_val),\n",
        "        #         callbacks=callbacks_list,\n",
        "        #         verbose=1\n",
        "        #     )\n",
        "        def make_ds(x_text, x_num, y, shuffle=True):\n",
        "            inputs = (x_text, x_num) if x_num is not None else x_text\n",
        "            ds = tf.data.Dataset.from_tensor_slices((inputs, y))\n",
        "            if shuffle:\n",
        "                ds = ds.shuffle(100_000)\n",
        "            return ds.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        train_ds = make_ds(*train_data, shuffle=True)\n",
        "        val_ds   = make_ds(*val_data,   shuffle=False)\n",
        "\n",
        "        with self.env_manager.strategy.scope():\n",
        "            self.history = self.model.fit(\n",
        "                train_ds,\n",
        "                epochs=epochs,\n",
        "                validation_data=val_ds,\n",
        "                callbacks=callbacks_list,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "        self.training_time = time.time() - start_time\n",
        "        logger.info(f\"Training completed in {self.training_time:.2f} seconds\")\n",
        "\n",
        "        return self.history.history\n",
        "\n",
        "\n",
        "def evaluate_pipeline_models_gpu(data_path: str,\n",
        "                                sample_fraction: float = 1.0,\n",
        "                                epochs: int = 10) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate models using MICAP pipeline data with GPU/TPU optimization.\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to processed pipeline data (parquet)\n",
        "        sample_fraction: Fraction of data to use\n",
        "        epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting pipeline-compatible GPU evaluation...\")\n",
        "\n",
        "    # Initialize components\n",
        "    env_manager = GPUEnvironmentManager()\n",
        "    data_loader = PipelineDataLoader()\n",
        "\n",
        "    # Load pipeline data\n",
        "    df = data_loader.load_pipeline_data(data_path, sample_fraction)\n",
        "\n",
        "    # Prepare features\n",
        "    texts, labels, numeric_features, feature_info = data_loader.prepare_features_for_training(df)\n",
        "\n",
        "    logger.info(f\"Loaded pipeline data: {feature_info}\")\n",
        "\n",
        "    # Initialize models\n",
        "    models_to_evaluate = [\n",
        "        ('Pipeline_LSTM', PipelineLSTMModel)\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model_name, ModelClass in models_to_evaluate:\n",
        "        logger.info(f\"\\n{'='*60}\")\n",
        "        logger.info(f\"Training {model_name}\")\n",
        "        logger.info(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Initialize model\n",
        "            model = ModelClass(env_manager)\n",
        "\n",
        "            # Prepare data\n",
        "            train_data, val_data = model.prepare_data(\n",
        "                texts, labels, numeric_features\n",
        "            )\n",
        "\n",
        "            # Build model\n",
        "            model.build_model()\n",
        "\n",
        "            # Train model\n",
        "            history = model.train(train_data, val_data, epochs=epochs)\n",
        "\n",
        "            # Evaluate\n",
        "            if model.numeric_features_dim > 0:\n",
        "                val_inputs = [val_data[0], val_data[1]]\n",
        "                val_labels = val_data[2]\n",
        "            else:\n",
        "                val_inputs = val_data[0]\n",
        "                val_labels = val_data[1]\n",
        "\n",
        "            eval_results = model.model.evaluate(val_inputs, val_labels, verbose=0)\n",
        "\n",
        "            metrics = {}\n",
        "            for i, metric_name in enumerate(model.model.metrics_names):\n",
        "                metrics[metric_name] = eval_results[i]\n",
        "            metrics['training_time'] = model.training_time\n",
        "\n",
        "            results[model_name] = {\n",
        "                'metrics': metrics,\n",
        "                'history': history,\n",
        "                'feature_info': feature_info\n",
        "            }\n",
        "\n",
        "            # Save model\n",
        "            model.model.save(f'/content/{model_name.lower()}_model.h5')\n",
        "            logger.info(f\"Model saved as {model_name.lower()}_model.h5\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to train {model_name}: {e}\")\n",
        "            results[model_name] = {'error': str(e)}\n",
        "\n",
        "    # Save results\n",
        "    with open('pipeline_model_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "    logger.info(\"Pipeline evaluation completed!\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# Usage examples for Google Colab\n",
        "# if __name__ == \"__main__\":\n",
        "#     \"\"\"\n",
        "#     Example usage in Google Colab:\n",
        "\n",
        "#     # Option 1: Use your existing pipeline data (RECOMMENDED)\n",
        "#     results = evaluate_pipeline_models_gpu(\n",
        "#         data_path='/content/pipeline_features.parquet',  # Upload your processed parquet file\n",
        "#         sample_fraction=1.0,\n",
        "#         epochs=10\n",
        "#     )\n",
        "\n",
        "#     # Option 2: If you have PySpark setup in Colab (advanced)\n",
        "#     # First run your preprocessing pipeline to create the parquet file\n",
        "#     # Then use the above approach\n",
        "#     \"\"\"\n",
        "\n",
        "#     results = evaluate_pipeline_models_gpu(\n",
        "#         'part-00000.snappy.parquet',\n",
        "#         sample_fraction=1.0,\n",
        "#         epochs=5\n",
        "#     )\n",
        "#     results = evaluate_pipeline_models_gpu(\n",
        "#         'part-00001.snappy.parquet',\n",
        "#         sample_fraction=1.0,\n",
        "#         epochs=5\n",
        "#     )\n",
        "#     results = evaluate_pipeline_models_gpu(\n",
        "#         'part-00002.snappy.parquet',\n",
        "#         sample_fraction=1.0,\n",
        "#         epochs=5\n",
        "#     )\n",
        "#     results = evaluate_pipeline_models_gpu(\n",
        "#         'part-00003.snappy.parquet',\n",
        "#         sample_fraction=1.0,\n",
        "#         epochs=5\n",
        "#     )\n",
        "\n",
        "#     logger.info(\"Pipeline-compatible GPU deep learning models ready!\")\n",
        "#     logger.info(\"Upload your 'pipeline_features.parquet' file to /content/ and run evaluation.\")\n",
        "\n",
        "def test_pipeline_models_on_dataset(\n",
        "    model_files: List[str],\n",
        "    test_csv_path: str = '/content/testdata.manual.2009.06.14.csv',\n",
        "    save_results: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Test trained models on the Sentiment140 test dataset with TPU optimization.\n",
        "    \n",
        "    Args:\n",
        "        model_files: List of model file paths to test\n",
        "        test_csv_path: Path to test CSV file\n",
        "        save_results: Whether to save results to file\n",
        "        \n",
        "    Returns:\n",
        "        Dict: Comprehensive test results\n",
        "    \"\"\"\n",
        "    logger.info(\"=\"*60)\n",
        "    logger.info(\"STARTING MODEL EVALUATION ON TEST DATASET\")\n",
        "    logger.info(f\"TPU-Optimized for v6e1 with 173GB RAM\")\n",
        "    logger.info(\"=\"*60)\n",
        "    \n",
        "    # Initialize environment\n",
        "    env_manager = GPUEnvironmentManager()\n",
        "    data_loader = PipelineDataLoader()\n",
        "    evaluator = TestDataEvaluator(env_manager, data_loader)\n",
        "    \n",
        "    # Load test data\n",
        "    try:\n",
        "        test_df = evaluator.load_test_data(test_csv_path)\n",
        "    except:\n",
        "        # Try alternative path if default fails\n",
        "        logger.warning(\"Default test path failed, trying alternative...\")\n",
        "        test_df = evaluator.load_test_data('/content/drive/MyDrive/Uni/BD/testdata.manual.2009.06.14.csv')\n",
        "    \n",
        "    # Store all results\n",
        "    all_results = []\n",
        "    \n",
        "    # Evaluate each model\n",
        "    for model_file in model_files:\n",
        "        if not os.path.exists(model_file):\n",
        "            logger.warning(f\"Model file not found: {model_file}\")\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            # Extract model name from filename\n",
        "            model_name = os.path.basename(model_file).replace('.h5', '').replace('best_', '').upper()\n",
        "            \n",
        "            logger.info(f\"\\n{'='*50}\")\n",
        "            logger.info(f\"Evaluating: {model_name}\")\n",
        "            logger.info(f\"{'='*50}\")\n",
        "            \n",
        "            # Load model to get configuration\n",
        "            with env_manager.strategy.scope():\n",
        "                model = tf.keras.models.load_model(model_file)\n",
        "                \n",
        "                # Get tokenizer configuration (you might need to save/load this separately)\n",
        "                # For now, we'll recreate it from the model's embedding layer\n",
        "                embedding_layer = model.get_layer('embedding')\n",
        "                max_words = embedding_layer.input_dim\n",
        "                max_length = embedding_layer.input_length or 100\n",
        "                \n",
        "                # Check if model uses numeric features\n",
        "                has_numeric = any('numeric_input' in layer.name for layer in model.layers)\n",
        "                numeric_dim = 0\n",
        "                if has_numeric:\n",
        "                    numeric_input_layer = model.get_layer('numeric_input')\n",
        "                    numeric_dim = numeric_input_layer.input_shape[-1]\n",
        "                \n",
        "            # Create tokenizer (in production, you'd save/load this with the model)\n",
        "            tokenizer = Tokenizer(\n",
        "                num_words=max_words,\n",
        "                oov_token='<OOV>',\n",
        "                filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "            )\n",
        "            \n",
        "            # Fit tokenizer on test data (in production, use saved tokenizer)\n",
        "            tokenizer.fit_on_texts(test_df['text_processed'].fillna('').astype(str))\n",
        "            \n",
        "            # Prepare test features\n",
        "            X_text, X_numeric, y_test = evaluator.prepare_test_features(\n",
        "                test_df, tokenizer, max_length, numeric_dim\n",
        "            )\n",
        "            \n",
        "            # Create test data tuple\n",
        "            if X_numeric is not None:\n",
        "                test_data = (X_text, X_numeric, y_test)\n",
        "            else:\n",
        "                test_data = (X_text, y_test)\n",
        "            \n",
        "            # Evaluate model\n",
        "            results = evaluator.evaluate_model(model_file, test_data, model_name)\n",
        "            all_results.append(results)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to evaluate {model_file}: {e}\")\n",
        "            all_results.append({\n",
        "                'model_name': model_name,\n",
        "                'error': str(e)\n",
        "            })\n",
        "    \n",
        "    # Create visualizations\n",
        "    if all_results:\n",
        "        evaluator.visualize_results(all_results)\n",
        "    \n",
        "    # Save results\n",
        "    if save_results:\n",
        "        results_file = '/content/test_results_comprehensive.json'\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(all_results, f, indent=2, default=str)\n",
        "        logger.info(f\"Results saved to {results_file}\")\n",
        "    \n",
        "    # Print summary\n",
        "    logger.info(\"\\n\" + \"=\"*60)\n",
        "    logger.info(\"TEST EVALUATION SUMMARY\")\n",
        "    logger.info(\"=\"*60)\n",
        "    \n",
        "    for result in all_results:\n",
        "        if 'error' not in result:\n",
        "            logger.info(f\"\\n{result['model_name']}:\")\n",
        "            logger.info(f\"  Test Accuracy: {result['test_accuracy']:.4f}\")\n",
        "            logger.info(f\"  Test Loss: {result['test_loss']:.4f}\")\n",
        "            if result.get('test_auc'):\n",
        "                logger.info(f\"  Test AUC: {result['test_auc']:.4f}\")\n",
        "            logger.info(f\"  F1-Score: {result['classification_report']['weighted avg']['f1-score']:.4f}\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# Usage example - add this at the very end of your notebook\n",
        "if __name__ == \"__main__\":\n",
        "    # After training your models, test them\n",
        "    trained_models = [\n",
        "        '/content/best_LSTM_pipeline_model.h5',\n",
        "        # Add other model paths here as you implement CNN and Transformer\n",
        "        # '/content/best_CNN_pipeline_model.h5',\n",
        "        # '/content/best_Transformer_pipeline_model.h5'\n",
        "    ]\n",
        "    \n",
        "    # Run comprehensive testing\n",
        "    test_results = test_pipeline_models_on_dataset(\n",
        "        model_files=trained_models,\n",
        "        test_csv_path='/content/testdata.manual.2009.06.14.csv'\n",
        "    )\n",
        "    \n",
        "    logger.info(\"\\n✅ Testing completed! Check visualization and results file.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
