{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHMu6EWGbv44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6dc1d1-33dd-44a6-e7f2-48fd4bedecfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "cp: cannot stat '/content/drive/MyDrive/Uni/BD/ppart-00001-a1634db0-8c09-4cb9-b1ca-466599ba8394-c000.snappy.parque': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow>=2.13.0 transformers matplotlib seaborn scikit-learn\n",
        "\n",
        "# Connect to my google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# copy files from google drive to project\n",
        "!cp '/content/drive/MyDrive/Uni/BD/part-00000-a1634db0-8c09-4cb9-b1ca-466599ba8394-c000.snappy.parquet' '/content/'\n",
        "!cp '/content/drive/MyDrive/Uni/BD/ppart-00001-a1634db0-8c09-4cb9-b1ca-466599ba8394-c000.snappy.parque' '/content/'\n",
        "!cp '/content/drive/MyDrive/Uni/BD/part-00002-a1634db0-8c09-4cb9-b1ca-466599ba8394-c000.snappy.parquet' '/content/'\n",
        "!cp '/content/drive/MyDrive/Uni/BD/part-00003-a1634db0-8c09-4cb9-b1ca-466599ba8394-c000.snappy.parquet' '/content/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previous"
      ],
      "metadata": {
        "id": "YZ9J3FXo4zfe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "V3wAIJODbqrR",
        "outputId": "a4b0a7d8-6f09-4262-99bb-c7ae2a85eb7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:No accelerator detected, using CPU\n",
            "ERROR:__main__:Failed to load data: [Errno 2] No such file or directory: '/content/sentiment_data.csv'\n",
            "ERROR:__main__:Evaluation failed: [Errno 2] No such file or directory: '/content/sentiment_data.csv'\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sentiment_data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1fa611436d46>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;31m# Example: evaluate with sample data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         results = evaluate_all_models_gpu(\n\u001b[0m\u001b[1;32m   1041\u001b[0m             \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/sentiment_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Update this path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0msample_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Use full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1fa611436d46>\u001b[0m in \u001b[0;36mevaluate_all_models_gpu\u001b[0;34m(data_path, sample_fraction, epochs)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading data from {data_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded dataset with shape: {df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sentiment_data.csv'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Deep Learning Models for Sentiment Analysis - GPU/TPU Optimized for Google Colab\n",
        "Implements LSTM, CNN, and Transformer models with CUDA/TPU acceleration\n",
        "Optimized for Google Colab environment with mixed precision training\n",
        "\n",
        "Author: AI Assistant\n",
        "Date: 2025-01-20\n",
        "Dependencies: tensorflow>=2.13.0, transformers, pandas, numpy\n",
        "Environment: Google Colab with GPU/TPU runtime\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import time\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# TensorFlow imports with GPU optimization\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
        "\n",
        "# Additional ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For distributed training and TPU support\n",
        "try:\n",
        "    from transformers import TFAutoModel, AutoTokenizer\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    logging.warning(\"Transformers library not available. Transformer models will be disabled.\")\n",
        "\n",
        "# Configure warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Configure logging for Colab\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler()  # Colab displays stdout/stderr inline\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GPUEnvironmentManager:\n",
        "    \"\"\"\n",
        "    Manages GPU/TPU environment setup and optimization for Google Colab\n",
        "    Handles device detection, memory management, and mixed precision setup\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize GPU environment manager with optimal settings.\"\"\"\n",
        "        self.device_type = self._detect_accelerator()\n",
        "        self.strategy = self._setup_distribution_strategy()\n",
        "        self.mixed_precision_enabled = False\n",
        "\n",
        "        # Log environment info\n",
        "        self._log_environment_info()\n",
        "\n",
        "        # Setup mixed precision if GPU available\n",
        "        if self.device_type == 'GPU':\n",
        "            self._setup_mixed_precision()\n",
        "\n",
        "    def _detect_accelerator(self) -> str:\n",
        "        \"\"\"\n",
        "        Detect available accelerator (GPU/TPU/CPU) in Google Colab.\n",
        "\n",
        "        Returns:\n",
        "            str: Device type ('GPU', 'TPU', or 'CPU')\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Check for TPU\n",
        "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            logger.info(\"TPU detected and initialized\")\n",
        "            return 'TPU'\n",
        "        except (ValueError, RuntimeError):\n",
        "            pass\n",
        "\n",
        "        # Check for GPU\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            logger.info(f\"GPU detected: {len(gpus)} device(s)\")\n",
        "            # Configure GPU memory growth to prevent OOM\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            return 'GPU'\n",
        "\n",
        "        logger.warning(\"No accelerator detected, using CPU\")\n",
        "        return 'CPU'\n",
        "\n",
        "    def _setup_distribution_strategy(self):\n",
        "        \"\"\"\n",
        "        Setup appropriate distribution strategy based on available hardware.\n",
        "\n",
        "        Returns:\n",
        "            tf.distribute.Strategy: Configured strategy for training\n",
        "        \"\"\"\n",
        "        if self.device_type == 'TPU':\n",
        "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "            strategy = tf.distribute.TPUStrategy(tpu)\n",
        "            logger.info(f\"Using TPU strategy with {strategy.num_replicas_in_sync} replicas\")\n",
        "        elif self.device_type == 'GPU':\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "            logger.info(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} replicas\")\n",
        "        else:\n",
        "            strategy = tf.distribute.get_strategy()  # Default strategy for CPU\n",
        "            logger.info(\"Using default strategy (CPU)\")\n",
        "\n",
        "        return strategy\n",
        "\n",
        "    def _setup_mixed_precision(self):\n",
        "        \"\"\"Setup mixed precision training for faster GPU training.\"\"\"\n",
        "        try:\n",
        "            # Enable mixed precision for faster training on modern GPUs\n",
        "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "            tf.keras.mixed_precision.set_global_policy(policy)\n",
        "            self.mixed_precision_enabled = True\n",
        "            logger.info(\"Mixed precision training enabled (float16)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not enable mixed precision: {e}\")\n",
        "            self.mixed_precision_enabled = False\n",
        "\n",
        "    def _log_environment_info(self):\n",
        "        \"\"\"Log comprehensive environment information for debugging.\"\"\"\n",
        "        logger.info(\"=== GPU/TPU Environment Information ===\")\n",
        "        logger.info(f\"TensorFlow version: {tf.__version__}\")\n",
        "        logger.info(f\"Detected accelerator: {self.device_type}\")\n",
        "\n",
        "        if self.device_type == 'GPU':\n",
        "            gpus = tf.config.list_physical_devices('GPU')\n",
        "            for i, gpu in enumerate(gpus):\n",
        "                logger.info(f\"GPU {i}: {gpu}\")\n",
        "                # Get GPU memory info\n",
        "                try:\n",
        "                    memory_info = tf.config.experimental.get_memory_info(gpu.name.replace('/physical_device:', ''))\n",
        "                    logger.info(f\"  Memory - Current: {memory_info['current']//1024//1024}MB, \"\n",
        "                              f\"Peak: {memory_info['peak']//1024//1024}MB\")\n",
        "                except:\n",
        "                    logger.info(\"  Memory info not available\")\n",
        "\n",
        "        logger.info(f\"Available logical devices: {len(tf.config.list_logical_devices())}\")\n",
        "        logger.info(\"=\" * 50)\n",
        "\n",
        "    def get_optimal_batch_size(self, base_batch_size: int = 32) -> int:\n",
        "        \"\"\"\n",
        "        Calculate optimal batch size based on available hardware.\n",
        "\n",
        "        Args:\n",
        "            base_batch_size: Base batch size for CPU/single GPU\n",
        "\n",
        "        Returns:\n",
        "            int: Optimized batch size\n",
        "        \"\"\"\n",
        "        if self.device_type == 'TPU':\n",
        "            # TPU works best with larger batch sizes (multiple of 8)\n",
        "            return max(128, base_batch_size * 8)\n",
        "        elif self.device_type == 'GPU':\n",
        "            # GPU can handle moderate batch sizes\n",
        "            return base_batch_size * max(1, self.strategy.num_replicas_in_sync)\n",
        "        else:\n",
        "            # CPU - smaller batch sizes\n",
        "            return max(16, base_batch_size // 2)\n",
        "\n",
        "\n",
        "class OptimizedDataLoader:\n",
        "    \"\"\"\n",
        "    Optimized data loading for Google Colab with GPU/TPU acceleration\n",
        "    Handles efficient data preprocessing and batching\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_manager: GPUEnvironmentManager):\n",
        "        \"\"\"\n",
        "        Initialize data loader with environment-specific optimizations.\n",
        "\n",
        "        Args:\n",
        "            env_manager: GPU environment manager instance\n",
        "        \"\"\"\n",
        "        self.env_manager = env_manager\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def prepare_colab_data(self, df: pd.DataFrame,\n",
        "                          text_col: str = 'text',\n",
        "                          label_col: str = 'sentiment',\n",
        "                          max_words: int = 10000,\n",
        "                          max_length: int = 100,\n",
        "                          validation_split: float = 0.2) -> Tuple[tf.data.Dataset, tf.data.Dataset, Tokenizer]:\n",
        "        \"\"\"\n",
        "        Prepare data optimized for Colab GPU/TPU training.\n",
        "\n",
        "        Args:\n",
        "            df: Input DataFrame with text and labels\n",
        "            text_col: Name of text column\n",
        "            label_col: Name of label column\n",
        "            max_words: Maximum vocabulary size\n",
        "            max_length: Maximum sequence length\n",
        "            validation_split: Fraction for validation set\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (train_dataset, val_dataset, tokenizer)\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing data for GPU/TPU training...\")\n",
        "        logger.info(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "        # Ensure we have the required columns\n",
        "        if text_col not in df.columns or label_col not in df.columns:\n",
        "            raise ValueError(f\"Required columns {text_col} and {label_col} not found\")\n",
        "\n",
        "        # Extract text and labels\n",
        "        texts = df[text_col].fillna('').astype(str).values\n",
        "        labels = df[label_col].values\n",
        "\n",
        "        logger.info(f\"Processing {len(texts)} text samples\")\n",
        "\n",
        "        # Initialize and fit tokenizer\n",
        "        self.tokenizer = Tokenizer(\n",
        "            num_words=max_words,\n",
        "            oov_token='<OOV>',\n",
        "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "        )\n",
        "\n",
        "        logger.info(\"Fitting tokenizer on text data...\")\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "\n",
        "        # Convert texts to sequences\n",
        "        logger.info(\"Converting texts to sequences...\")\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "        # Pad sequences\n",
        "        logger.info(f\"Padding sequences to max length {max_length}...\")\n",
        "        X = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "        y = labels.astype(np.float32)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y, test_size=validation_split, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Train set: {X_train.shape[0]} samples\")\n",
        "        logger.info(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "\n",
        "        # Calculate optimal batch size\n",
        "        batch_size = self.env_manager.get_optimal_batch_size()\n",
        "        logger.info(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "        # Create TensorFlow datasets with optimization\n",
        "        train_dataset = self._create_optimized_dataset(\n",
        "            X_train, y_train, batch_size, shuffle=True, repeat=True\n",
        "        )\n",
        "        val_dataset = self._create_optimized_dataset(\n",
        "            X_val, y_val, batch_size, shuffle=False, repeat=False\n",
        "        )\n",
        "\n",
        "        return train_dataset, val_dataset, self.tokenizer\n",
        "\n",
        "    def _create_optimized_dataset(self, X: np.ndarray, y: np.ndarray,\n",
        "                                batch_size: int, shuffle: bool = True,\n",
        "                                repeat: bool = False) -> tf.data.Dataset:\n",
        "        \"\"\"\n",
        "        Create optimized TensorFlow dataset for training.\n",
        "\n",
        "        Args:\n",
        "            X: Input sequences\n",
        "            y: Labels\n",
        "            batch_size: Batch size\n",
        "            shuffle: Whether to shuffle data\n",
        "            repeat: Whether to repeat dataset\n",
        "\n",
        "        Returns:\n",
        "            tf.data.Dataset: Optimized dataset\n",
        "        \"\"\"\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "        if shuffle:\n",
        "            # Use larger buffer for better shuffling\n",
        "            buffer_size = min(10000, len(X))\n",
        "            dataset = dataset.shuffle(buffer_size)\n",
        "\n",
        "        if repeat:\n",
        "            dataset = dataset.repeat()\n",
        "\n",
        "        dataset = dataset.batch(batch_size)\n",
        "\n",
        "        # Optimization for performance\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        if self.env_manager.device_type in ['GPU', 'TPU']:\n",
        "            # Additional optimizations for accelerators\n",
        "            dataset = dataset.cache()  # Cache preprocessed data\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "class OptimizedDeepLearningModel:\n",
        "    \"\"\"\n",
        "    Base class for GPU/TPU optimized deep learning models\n",
        "    Includes mixed precision training, model checkpointing, and advanced callbacks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_manager: GPUEnvironmentManager,\n",
        "                 max_words: int = 10000, max_length: int = 100):\n",
        "        \"\"\"\n",
        "        Initialize optimized deep learning model.\n",
        "\n",
        "        Args:\n",
        "            env_manager: GPU environment manager\n",
        "            max_words: Maximum vocabulary size\n",
        "            max_length: Maximum sequence length\n",
        "        \"\"\"\n",
        "        self.env_manager = env_manager\n",
        "        self.max_words = max_words\n",
        "        self.max_length = max_length\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.training_time = 0\n",
        "\n",
        "    def _get_optimizer(self, learning_rate: float = 0.001):\n",
        "        \"\"\"\n",
        "        Get optimized optimizer for the current hardware setup.\n",
        "\n",
        "        Args:\n",
        "            learning_rate: Learning rate for optimizer\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.optimizers.Optimizer: Configured optimizer\n",
        "        \"\"\"\n",
        "        if self.env_manager.device_type == 'TPU':\n",
        "            # TPU works well with higher learning rates\n",
        "            optimizer = optimizers.Adam(learning_rate=learning_rate * 2)\n",
        "        else:\n",
        "            optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        # Wrap with loss scale optimizer if mixed precision is enabled\n",
        "        if self.env_manager.mixed_precision_enabled:\n",
        "            optimizer = LossScaleOptimizer(optimizer)\n",
        "            logger.info(\"Using mixed precision optimizer\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def _get_callbacks(self, model_name: str, patience: int = 5) -> List[callbacks.Callback]:\n",
        "        \"\"\"\n",
        "        Get optimized callbacks for training.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the model for checkpointing\n",
        "            patience: Patience for early stopping\n",
        "\n",
        "        Returns:\n",
        "            List of configured callbacks\n",
        "        \"\"\"\n",
        "        callback_list = []\n",
        "\n",
        "        # Early stopping\n",
        "        early_stop = callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=patience,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1,\n",
        "            mode='min'\n",
        "        )\n",
        "        callback_list.append(early_stop)\n",
        "\n",
        "        # Learning rate reduction\n",
        "        reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=max(2, patience // 2),\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "        callback_list.append(reduce_lr)\n",
        "\n",
        "        # Model checkpointing (save to Colab's temporary storage)\n",
        "        checkpoint_path = f'/content/best_{model_name}_model.h5'\n",
        "        model_checkpoint = callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_path,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            verbose=1\n",
        "        )\n",
        "        callback_list.append(model_checkpoint)\n",
        "\n",
        "        # Progress tracking for Colab\n",
        "        class ColabProgressCallback(callbacks.Callback):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.start_time = time.time()\n",
        "\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                elapsed_time = time.time() - self.start_time\n",
        "                logs = logs or {}\n",
        "                logger.info(f\"Epoch {epoch + 1} - Loss: {logs.get('loss', 0):.4f} - \"\n",
        "                          f\"Val Loss: {logs.get('val_loss', 0):.4f} - \"\n",
        "                          f\"Accuracy: {logs.get('accuracy', 0):.4f} - \"\n",
        "                          f\"Time: {elapsed_time:.1f}s\")\n",
        "\n",
        "        callback_list.append(ColabProgressCallback())\n",
        "\n",
        "        return callback_list\n",
        "\n",
        "    def compile_model(self):\n",
        "        \"\"\"Compile model with appropriate settings for the hardware.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built yet. Call build_model() first.\")\n",
        "\n",
        "        optimizer = self._get_optimizer()\n",
        "\n",
        "        # Configure loss and metrics\n",
        "        loss = 'binary_crossentropy'\n",
        "        metrics = ['accuracy']\n",
        "\n",
        "        # Add AUC metric for better evaluation\n",
        "        if hasattr(tf.keras.metrics, 'AUC'):\n",
        "            metrics.append(tf.keras.metrics.AUC(name='auc'))\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=loss,\n",
        "            metrics=metrics\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Model compiled with {optimizer.__class__.__name__} optimizer\")\n",
        "\n",
        "    def train_model(self, train_dataset: tf.data.Dataset,\n",
        "                   val_dataset: tf.data.Dataset,\n",
        "                   epochs: int = 10,\n",
        "                   steps_per_epoch: Optional[int] = None,\n",
        "                   validation_steps: Optional[int] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Train model with GPU/TPU optimization.\n",
        "\n",
        "        Args:\n",
        "            train_dataset: Training dataset\n",
        "            val_dataset: Validation dataset\n",
        "            epochs: Number of epochs\n",
        "            steps_per_epoch: Steps per epoch (auto-calculated if None)\n",
        "            validation_steps: Validation steps (auto-calculated if None)\n",
        "\n",
        "        Returns:\n",
        "            Dict: Training history\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
        "\n",
        "        logger.info(f\"Starting training for {epochs} epochs...\")\n",
        "\n",
        "        # Get callbacks\n",
        "        callbacks_list = self._get_callbacks(self.__class__.__name__)\n",
        "\n",
        "        # Record training start time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train model within distribution strategy scope\n",
        "        with self.env_manager.strategy.scope():\n",
        "            self.history = self.model.fit(\n",
        "                train_dataset,\n",
        "                epochs=epochs,\n",
        "                validation_data=val_dataset,\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "                validation_steps=validation_steps,\n",
        "                callbacks=callbacks_list,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "        # Record training time\n",
        "        self.training_time = time.time() - start_time\n",
        "        logger.info(f\"Training completed in {self.training_time:.2f} seconds\")\n",
        "\n",
        "        return self.history.history\n",
        "\n",
        "    def evaluate_model(self, test_dataset: tf.data.Dataset) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate model performance.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: Test dataset\n",
        "\n",
        "        Returns:\n",
        "            Dict: Evaluation metrics\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet.\")\n",
        "\n",
        "        logger.info(\"Evaluating model...\")\n",
        "\n",
        "        # Evaluate model\n",
        "        eval_results = self.model.evaluate(test_dataset, verbose=1)\n",
        "\n",
        "        # Create results dictionary\n",
        "        metrics = {}\n",
        "        for i, metric_name in enumerate(self.model.metrics_names):\n",
        "            metrics[metric_name] = eval_results[i]\n",
        "\n",
        "        # Add training time\n",
        "        metrics['training_time'] = self.training_time\n",
        "\n",
        "        logger.info(\"Evaluation Results:\")\n",
        "        for metric, value in metrics.items():\n",
        "            logger.info(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "class OptimizedLSTMModel(OptimizedDeepLearningModel):\n",
        "    \"\"\"\n",
        "    GPU/TPU optimized LSTM model for sentiment analysis\n",
        "    Includes bidirectional LSTM layers and advanced regularization\n",
        "    \"\"\"\n",
        "\n",
        "    def build_model(self, embedding_dim: int = 128,\n",
        "                   lstm_units: int = 64,\n",
        "                   dropout_rate: float = 0.3) -> models.Model:\n",
        "        \"\"\"\n",
        "        Build optimized LSTM model architecture.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim: Dimension of word embeddings\n",
        "            lstm_units: Number of LSTM units\n",
        "            dropout_rate: Dropout rate for regularization\n",
        "\n",
        "        Returns:\n",
        "            Compiled Keras model\n",
        "        \"\"\"\n",
        "        logger.info(\"Building optimized LSTM model...\")\n",
        "\n",
        "        with self.env_manager.strategy.scope():\n",
        "            model = models.Sequential([\n",
        "                # Embedding layer with mask_zero for padding\n",
        "                layers.Embedding(\n",
        "                    input_dim=self.max_words,\n",
        "                    output_dim=embedding_dim,\n",
        "                    input_length=self.max_length,\n",
        "                    mask_zero=True,\n",
        "                    name='embedding'\n",
        "                ),\n",
        "\n",
        "                # Spatial dropout for embedding regularization\n",
        "                layers.SpatialDropout1D(dropout_rate * 0.5),\n",
        "\n",
        "                # Bidirectional LSTM for better context understanding\n",
        "                layers.Bidirectional(\n",
        "                    layers.LSTM(\n",
        "                        lstm_units,\n",
        "                        dropout=dropout_rate,\n",
        "                        recurrent_dropout=dropout_rate * 0.5,\n",
        "                        return_sequences=True\n",
        "                    ),\n",
        "                    name='bi_lstm_1'\n",
        "                ),\n",
        "\n",
        "                # Second LSTM layer for deeper representation\n",
        "                layers.Bidirectional(\n",
        "                    layers.LSTM(\n",
        "                        lstm_units // 2,\n",
        "                        dropout=dropout_rate,\n",
        "                        recurrent_dropout=dropout_rate * 0.5,\n",
        "                        return_sequences=False\n",
        "                    ),\n",
        "                    name='bi_lstm_2'\n",
        "                ),\n",
        "\n",
        "                # Dense layers with batch normalization\n",
        "                layers.Dense(64, activation='relu', name='dense_1'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.Dropout(dropout_rate),\n",
        "\n",
        "                layers.Dense(32, activation='relu', name='dense_2'),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.Dropout(dropout_rate * 0.5),\n",
        "\n",
        "                # Output layer\n",
        "                layers.Dense(1, activation='sigmoid', name='output')\n",
        "            ])\n",
        "\n",
        "        self.model = model\n",
        "        self.compile_model()\n",
        "\n",
        "        # Log model summary\n",
        "        logger.info(\"LSTM Model Architecture:\")\n",
        "        self.model.summary(print_fn=logger.info)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class OptimizedCNNModel(OptimizedDeepLearningModel):\n",
        "    \"\"\"\n",
        "    GPU/TPU optimized CNN model for sentiment analysis\n",
        "    Uses multiple filter sizes for better n-gram capture\n",
        "    \"\"\"\n",
        "\n",
        "    def build_model(self, embedding_dim: int = 128,\n",
        "                   filter_sizes: List[int] = [3, 4, 5],\n",
        "                   num_filters: int = 100,\n",
        "                   dropout_rate: float = 0.5) -> models.Model:\n",
        "        \"\"\"\n",
        "        Build optimized CNN model with multiple filter sizes.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim: Dimension of word embeddings\n",
        "            filter_sizes: List of filter sizes for convolution\n",
        "            num_filters: Number of filters per filter size\n",
        "            dropout_rate: Dropout rate for regularization\n",
        "\n",
        "        Returns:\n",
        "            Compiled Keras model\n",
        "        \"\"\"\n",
        "        logger.info(\"Building optimized CNN model...\")\n",
        "\n",
        "        with self.env_manager.strategy.scope():\n",
        "            # Input layer\n",
        "            inputs = layers.Input(shape=(self.max_length,), name='input')\n",
        "\n",
        "            # Embedding layer\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=self.max_words,\n",
        "                output_dim=embedding_dim,\n",
        "                input_length=self.max_length,\n",
        "                mask_zero=False,  # CNN doesn't work well with masking\n",
        "                name='embedding'\n",
        "            )(inputs)\n",
        "\n",
        "            embedding = layers.SpatialDropout1D(dropout_rate * 0.3)(embedding)\n",
        "\n",
        "            # Multiple CNN branches with different filter sizes\n",
        "            conv_blocks = []\n",
        "            for filter_size in filter_sizes:\n",
        "                conv = layers.Conv1D(\n",
        "                    filters=num_filters,\n",
        "                    kernel_size=filter_size,\n",
        "                    activation='relu',\n",
        "                    padding='valid',\n",
        "                    name=f'conv1d_{filter_size}'\n",
        "                )(embedding)\n",
        "\n",
        "                # Batch normalization for training stability\n",
        "                conv = layers.BatchNormalization()(conv)\n",
        "\n",
        "                # Global max pooling to capture most important features\n",
        "                conv = layers.GlobalMaxPooling1D(name=f'maxpool_{filter_size}')(conv)\n",
        "\n",
        "                conv_blocks.append(conv)\n",
        "\n",
        "            # Concatenate all conv blocks\n",
        "            if len(conv_blocks) > 1:\n",
        "                merged = layers.Concatenate(name='concat')(conv_blocks)\n",
        "            else:\n",
        "                merged = conv_blocks[0]\n",
        "\n",
        "            # Dense layers for classification\n",
        "            dense = layers.Dense(128, activation='relu', name='dense_1')(merged)\n",
        "            dense = layers.BatchNormalization()(dense)\n",
        "            dense = layers.Dropout(dropout_rate)(dense)\n",
        "\n",
        "            dense = layers.Dense(64, activation='relu', name='dense_2')(dense)\n",
        "            dense = layers.BatchNormalization()(dense)\n",
        "            dense = layers.Dropout(dropout_rate * 0.5)(dense)\n",
        "\n",
        "            # Output layer\n",
        "            outputs = layers.Dense(1, activation='sigmoid', name='output')(dense)\n",
        "\n",
        "            model = models.Model(inputs=inputs, outputs=outputs, name='OptimizedCNN')\n",
        "\n",
        "        self.model = model\n",
        "        self.compile_model()\n",
        "\n",
        "        # Log model summary\n",
        "        logger.info(\"CNN Model Architecture:\")\n",
        "        self.model.summary(print_fn=logger.info)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class OptimizedTransformerModel(OptimizedDeepLearningModel):\n",
        "    \"\"\"\n",
        "    GPU/TPU optimized Transformer model for sentiment analysis\n",
        "    Uses multi-head attention with positional encoding\n",
        "    \"\"\"\n",
        "\n",
        "    def build_model(self, embedding_dim: int = 128,\n",
        "                   num_heads: int = 8,\n",
        "                   ff_dim: int = 256,\n",
        "                   num_transformer_blocks: int = 2,\n",
        "                   dropout_rate: float = 0.1) -> models.Model:\n",
        "        \"\"\"\n",
        "        Build optimized Transformer model architecture.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim: Dimension of word embeddings\n",
        "            num_heads: Number of attention heads\n",
        "            ff_dim: Feed-forward network dimension\n",
        "            num_transformer_blocks: Number of transformer blocks\n",
        "            dropout_rate: Dropout rate for regularization\n",
        "\n",
        "        Returns:\n",
        "            Compiled Keras model\n",
        "        \"\"\"\n",
        "        logger.info(\"Building optimized Transformer model...\")\n",
        "\n",
        "        with self.env_manager.strategy.scope():\n",
        "            # Input layer\n",
        "            inputs = layers.Input(shape=(self.max_length,), name='input')\n",
        "\n",
        "            # Embedding layer\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=self.max_words,\n",
        "                output_dim=embedding_dim,\n",
        "                input_length=self.max_length,\n",
        "                mask_zero=True,\n",
        "                name='embedding'\n",
        "            )(inputs)\n",
        "\n",
        "            # Positional encoding\n",
        "            position_encoding = self._get_positional_encoding(\n",
        "                self.max_length, embedding_dim\n",
        "            )\n",
        "\n",
        "            # Add positional encoding to embeddings\n",
        "            x = embedding + position_encoding\n",
        "            x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "            # Transformer blocks\n",
        "            for i in range(num_transformer_blocks):\n",
        "                x = self._transformer_block(\n",
        "                    x, embedding_dim, num_heads, ff_dim, dropout_rate, f'transformer_{i}'\n",
        "                )\n",
        "\n",
        "            # Global average pooling\n",
        "            x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "            # Dense layers for classification\n",
        "            x = layers.Dense(128, activation='relu', name='dense_1')(x)\n",
        "            x = layers.BatchNormalization()(x)\n",
        "            x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "            x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
        "            x = layers.Dropout(dropout_rate * 0.5)(x)\n",
        "\n",
        "            # Output layer\n",
        "            outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "\n",
        "            model = models.Model(inputs=inputs, outputs=outputs, name='OptimizedTransformer')\n",
        "\n",
        "        self.model = model\n",
        "        self.compile_model()\n",
        "\n",
        "        # Log model summary\n",
        "        logger.info(\"Transformer Model Architecture:\")\n",
        "        self.model.summary(print_fn=logger.info)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _get_positional_encoding(self, seq_len: int, d_model: int):\n",
        "        \"\"\"\n",
        "        Create positional encoding for transformer.\n",
        "\n",
        "        Args:\n",
        "            seq_len: Sequence length\n",
        "            d_model: Model dimension\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Positional encoding tensor\n",
        "        \"\"\"\n",
        "        pos = np.arange(seq_len)[:, np.newaxis]\n",
        "        i = np.arange(d_model)[np.newaxis, :]\n",
        "\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        # Apply sin to even indices and cos to odd indices\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def _transformer_block(self, x, embed_dim: int, num_heads: int,\n",
        "                          ff_dim: int, dropout_rate: float, name: str):\n",
        "        \"\"\"\n",
        "        Create a transformer block.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            embed_dim: Embedding dimension\n",
        "            num_heads: Number of attention heads\n",
        "            ff_dim: Feed-forward dimension\n",
        "            dropout_rate: Dropout rate\n",
        "            name: Block name\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Output tensor\n",
        "        \"\"\"\n",
        "        # Multi-head attention\n",
        "        attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim,\n",
        "            name=f'{name}_attention'\n",
        "        )(x, x)\n",
        "\n",
        "        attention = layers.Dropout(dropout_rate)(attention)\n",
        "\n",
        "        # Add & Norm\n",
        "        attention = layers.Add(name=f'{name}_add_1')([x, attention])\n",
        "        attention = layers.LayerNormalization(name=f'{name}_norm_1')(attention)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ff = layers.Dense(ff_dim, activation='relu', name=f'{name}_ff_1')(attention)\n",
        "        ff = layers.Dropout(dropout_rate)(ff)\n",
        "        ff = layers.Dense(embed_dim, name=f'{name}_ff_2')(ff)\n",
        "        ff = layers.Dropout(dropout_rate)(ff)\n",
        "\n",
        "        # Add & Norm\n",
        "        output = layers.Add(name=f'{name}_add_2')([attention, ff])\n",
        "        output = layers.LayerNormalization(name=f'{name}_norm_2')(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def evaluate_all_models_gpu(data_path: str = '/content/data.csv',\n",
        "                           sample_fraction: float = 1.0,\n",
        "                           epochs: int = 10) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate all deep learning models with GPU/TPU optimization.\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to CSV data file\n",
        "        sample_fraction: Fraction of data to use (1.0 = full data)\n",
        "        epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        Dict: Comprehensive evaluation results\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting GPU/TPU optimized model evaluation...\")\n",
        "\n",
        "    # Initialize environment manager\n",
        "    env_manager = GPUEnvironmentManager()\n",
        "\n",
        "    # Load data\n",
        "    logger.info(f\"Loading data from {data_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(data_path)\n",
        "        logger.info(f\"Loaded dataset with shape: {df.shape}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load data: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Sample data if requested\n",
        "    if sample_fraction < 1.0:\n",
        "        df = df.sample(frac=sample_fraction, random_state=42)\n",
        "        logger.info(f\"Using {sample_fraction*100}% of data: {df.shape[0]} samples\")\n",
        "\n",
        "    # Initialize data loader\n",
        "    data_loader = OptimizedDataLoader(env_manager)\n",
        "\n",
        "    # Prepare data\n",
        "    train_dataset, val_dataset, tokenizer = data_loader.prepare_colab_data(df)\n",
        "\n",
        "    # Initialize models\n",
        "    models_to_evaluate = [\n",
        "        ('LSTM', OptimizedLSTMModel),\n",
        "        ('CNN', OptimizedCNNModel),\n",
        "        ('Transformer', OptimizedTransformerModel)\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Evaluate each model\n",
        "    for model_name, ModelClass in models_to_evaluate:\n",
        "        logger.info(f\"\\n{'='*60}\")\n",
        "        logger.info(f\"Training {model_name} Model\")\n",
        "        logger.info(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Initialize model\n",
        "            model = ModelClass(env_manager)\n",
        "\n",
        "            # Build model with appropriate parameters\n",
        "            if model_name == 'LSTM':\n",
        "                model.build_model(embedding_dim=128, lstm_units=64)\n",
        "            elif model_name == 'CNN':\n",
        "                model.build_model(embedding_dim=128, filter_sizes=[3, 4, 5], num_filters=100)\n",
        "            elif model_name == 'Transformer':\n",
        "                model.build_model(embedding_dim=128, num_heads=8, ff_dim=256)\n",
        "\n",
        "            # Train model\n",
        "            history = model.train_model(\n",
        "                train_dataset,\n",
        "                val_dataset,\n",
        "                epochs=epochs\n",
        "            )\n",
        "\n",
        "            # Evaluate model\n",
        "            metrics = model.evaluate_model(val_dataset)\n",
        "\n",
        "            # Store results\n",
        "            results[model_name] = {\n",
        "                'metrics': metrics,\n",
        "                'history': history,\n",
        "                'model_params': model.model.count_params()\n",
        "            }\n",
        "\n",
        "            logger.info(f\"{model_name} training completed successfully\")\n",
        "\n",
        "            # Save model for later use\n",
        "            model.model.save(f'/content/{model_name.lower()}_model.h5')\n",
        "            logger.info(f\"Model saved as {model_name.lower()}_model.h5\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to train {model_name}: {e}\")\n",
        "            results[model_name] = {\n",
        "                'metrics': {'error': str(e)},\n",
        "                'history': {},\n",
        "                'model_params': 0\n",
        "            }\n",
        "\n",
        "    # Create summary\n",
        "    logger.info(\"\\n\" + \"=\"*60)\n",
        "    logger.info(\"MODEL EVALUATION SUMMARY\")\n",
        "    logger.info(\"=\"*60)\n",
        "\n",
        "    summary_data = []\n",
        "    for model_name, result in results.items():\n",
        "        if 'error' not in result['metrics']:\n",
        "            summary_data.append({\n",
        "                'Model': model_name,\n",
        "                'Accuracy': result['metrics'].get('accuracy', 0),\n",
        "                'Loss': result['metrics'].get('loss', 0),\n",
        "                'AUC': result['metrics'].get('auc', 0),\n",
        "                'Training Time (s)': result['metrics'].get('training_time', 0),\n",
        "                'Parameters': result['model_params']\n",
        "            })\n",
        "\n",
        "    # Display results\n",
        "    if summary_data:\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_df = summary_df.sort_values('Accuracy', ascending=False)\n",
        "        logger.info(f\"\\n{summary_df.to_string(index=False)}\")\n",
        "\n",
        "        # Save results\n",
        "        results['summary'] = summary_df.to_dict('records')\n",
        "\n",
        "        # Save comprehensive results\n",
        "        with open('/content/model_evaluation_results.json', 'w') as f:\n",
        "            json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "        logger.info(\"Results saved to /content/model_evaluation_results.json\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_training_history(results: Dict, save_path: str = '/content/training_plots.png'):\n",
        "    \"\"\"\n",
        "    Plot training history for all models.\n",
        "\n",
        "    Args:\n",
        "        results: Results dictionary from evaluate_all_models_gpu\n",
        "        save_path: Path to save the plot\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    ax = axes[0, 0]\n",
        "    for model_name, result in results.items():\n",
        "        if model_name != 'summary' and 'history' in result:\n",
        "            history = result['history']\n",
        "            if 'loss' in history:\n",
        "                epochs = range(1, len(history['loss']) + 1)\n",
        "                ax.plot(epochs, history['loss'], label=f'{model_name} Train')\n",
        "                if 'val_loss' in history:\n",
        "                    ax.plot(epochs, history['val_loss'], label=f'{model_name} Val', linestyle='--')\n",
        "\n",
        "    ax.set_title('Model Loss')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    ax = axes[0, 1]\n",
        "    for model_name, result in results.items():\n",
        "        if model_name != 'summary' and 'history' in result:\n",
        "            history = result['history']\n",
        "            if 'accuracy' in history:\n",
        "                epochs = range(1, len(history['accuracy']) + 1)\n",
        "                ax.plot(epochs, history['accuracy'], label=f'{model_name} Train')\n",
        "                if 'val_accuracy' in history:\n",
        "                    ax.plot(epochs, history['val_accuracy'], label=f'{model_name} Val', linestyle='--')\n",
        "\n",
        "    ax.set_title('Model Accuracy')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Plot final metrics comparison\n",
        "    if 'summary' in results:\n",
        "        summary_df = pd.DataFrame(results['summary'])\n",
        "\n",
        "        ax = axes[1, 0]\n",
        "        ax.bar(summary_df['Model'], summary_df['Accuracy'], color=['blue', 'green', 'red'])\n",
        "        ax.set_title('Final Accuracy Comparison')\n",
        "        ax.set_ylabel('Accuracy')\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        ax = axes[1, 1]\n",
        "        ax.bar(summary_df['Model'], summary_df['Training Time (s)'], color=['blue', 'green', 'red'])\n",
        "        ax.set_title('Training Time Comparison')\n",
        "        ax.set_ylabel('Time (seconds)')\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    logger.info(f\"Training plots saved to {save_path}\")\n",
        "\n",
        "\n",
        "# Example usage for Google Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage - modify data_path to your actual data location\n",
        "    logger.info(\"Starting GPU/TPU optimized deep learning evaluation...\")\n",
        "\n",
        "    # Example: evaluate with sample data\n",
        "    try:\n",
        "        results = evaluate_all_models_gpu(\n",
        "            data_path='/content/sentiment_data.csv',  # Update this path\n",
        "            sample_fraction=1.0,  # Use full dataset\n",
        "            epochs=10\n",
        "        )\n",
        "\n",
        "        # Plot results\n",
        "        plot_training_history(results)\n",
        "\n",
        "        logger.info(\"Evaluation completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Evaluation failed: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now"
      ],
      "metadata": {
        "id": "AY9cpuzI48_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n2XPjFl5eG4y",
        "outputId": "95a2b8a9-7a8c-4b84-c5b1-cb914526e339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:PySpark not available. Will use pandas for data loading.\n",
            "WARNING:__main__:No accelerator detected, using CPU\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.5203 - auc: 0.0000e+00 - loss: 0.6988 - learning_rate: 0.0010\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,auc,loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:145: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,auc,loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/model_checkpoint.py:209: UserWarning: Can save best model only with val_loss available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.7907 - auc: 0.0000e+00 - loss: 0.5712 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.9076 - auc: 0.0000e+00 - loss: 0.4543 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.9593 - auc: 0.0000e+00 - loss: 0.3552 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.9822 - auc: 0.0000e+00 - loss: 0.2781 - learning_rate: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:__main__:No accelerator detected, using CPU\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5760 - auc: 0.4401 - loss: 0.6584\n",
            "Epoch 1: val_loss improved from inf to 0.17509, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5875 - auc: 0.4422 - loss: 0.6515 - val_accuracy: 0.9746 - val_auc: 0.4966 - val_loss: 0.1751 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8419 - auc: 0.4779 - loss: 0.4773\n",
            "Epoch 2: val_loss did not improve from 0.17509\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.8461 - auc: 0.4784 - loss: 0.4717 - val_accuracy: 0.9746 - val_auc: 0.4964 - val_loss: 0.1774 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9279 - auc: 0.4952 - loss: 0.3413\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 3: val_loss did not improve from 0.17509\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.9297 - auc: 0.4960 - loss: 0.3378 - val_accuracy: 0.9745 - val_auc: 0.4951 - val_loss: 0.1833 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9608 - auc: 0.5221 - loss: 0.2630\n",
            "Epoch 4: val_loss did not improve from 0.17509\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.9610 - auc: 0.5192 - loss: 0.2624 - val_accuracy: 0.9745 - val_auc: 0.4949 - val_loss: 0.1898 - learning_rate: 5.0000e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9691 - auc: 0.4961 - loss: 0.2365\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.17509\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.9691 - auc: 0.4969 - loss: 0.2359 - val_accuracy: 0.9741 - val_auc: 0.4943 - val_loss: 0.1971 - learning_rate: 5.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:__main__:No accelerator detected, using CPU\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.4702 - auc: 0.0000e+00 - loss: 0.7274 - learning_rate: 0.0010\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,auc,loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:145: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,auc,loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/model_checkpoint.py:209: UserWarning: Can save best model only with val_loss available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.6937 - auc: 0.0000e+00 - loss: 0.6174 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.7966 - auc: 0.0000e+00 - loss: 0.5237 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.8934 - auc: 0.0000e+00 - loss: 0.4252 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.9565 - auc: 0.0000e+00 - loss: 0.3346 - learning_rate: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:__main__:No accelerator detected, using CPU\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7784 - auc: 0.0000e+00 - loss: 0.5816\n",
            "Epoch 1: val_loss improved from inf to 0.10028, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7876 - auc: 0.0000e+00 - loss: 0.5752 - val_accuracy: 0.9999 - val_auc: 0.0000e+00 - val_loss: 0.1003 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9499 - auc: 0.0000e+00 - loss: 0.3985\n",
            "Epoch 2: val_loss improved from 0.10028 to 0.09630, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.9520 - auc: 0.0000e+00 - loss: 0.3915 - val_accuracy: 0.9999 - val_auc: 0.0000e+00 - val_loss: 0.0963 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9869 - auc: 0.0000e+00 - loss: 0.2464\n",
            "Epoch 3: val_loss improved from 0.09630 to 0.08983, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.9874 - auc: 0.0000e+00 - loss: 0.2426 - val_accuracy: 0.9999 - val_auc: 0.0000e+00 - val_loss: 0.0898 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9955 - auc: 0.0000e+00 - loss: 0.1559\n",
            "Epoch 4: val_loss improved from 0.08983 to 0.08560, saving model to /content/best_LSTM_pipeline_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.9957 - auc: 0.0000e+00 - loss: 0.1534 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0856 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9985 - auc: 0.0000e+00 - loss: 0.0989\n",
            "Epoch 5: val_loss did not improve from 0.08560\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.9985 - auc: 0.0000e+00 - loss: 0.0972 - val_accuracy: 1.0000 - val_auc: 0.0000e+00 - val_loss: 0.0858 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 4.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Deep Learning Models for Sentiment Analysis - GPU/TPU Optimized for Google Colab\n",
        "PIPELINE-COMPATIBLE VERSION - Works with existing MICAP preprocessing pipeline\n",
        "Implements LSTM, CNN, and Transformer models with CUDA/TPU acceleration\n",
        "\n",
        "Author: AI Assistant\n",
        "Date: 2025-01-20\n",
        "Dependencies: tensorflow>=2.13.0, pandas, numpy, pyspark (for data loading)\n",
        "Environment: Google Colab with GPU/TPU runtime\n",
        "Pipeline: Reads from preprocessed parquet files created by MICAP pipeline\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import time\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# TensorFlow imports with GPU optimization\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
        "\n",
        "# Additional ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# PySpark imports for reading existing pipeline data\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql.functions import col\n",
        "    PYSPARK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYSPARK_AVAILABLE = False\n",
        "    logging.warning(\"PySpark not available. Will use pandas for data loading.\")\n",
        "\n",
        "tf.config.optimizer.set_jit(True)      # XLA compilation\n",
        "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
        "\n",
        "# Configure warnings and logging\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class PipelineDataLoader:\n",
        "    \"\"\"\n",
        "    Loads data from MICAP preprocessing pipeline (parquet files)\n",
        "    Maintains compatibility with existing feature engineering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize pipeline data loader.\"\"\"\n",
        "        self.spark = None\n",
        "        if PYSPARK_AVAILABLE:\n",
        "            self._init_spark_session()\n",
        "\n",
        "    def _init_spark_session(self):\n",
        "        \"\"\"Initialize Spark session for reading parquet files.\"\"\"\n",
        "        try:\n",
        "            self.spark = (SparkSession.builder\n",
        "                         .appName(\"ColabDataLoader\")\n",
        "                         .master(\"local[*]\")\n",
        "                         .config(\"spark.driver.memory\", \"4g\")\n",
        "                         .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "                         .getOrCreate())\n",
        "            self.spark.sparkContext.setLogLevel(\"WARN\")\n",
        "            logger.info(\"Spark session created for pipeline data loading\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create Spark session: {e}\")\n",
        "            self.spark = None\n",
        "\n",
        "    def load_pipeline_data(self, data_path: str,\n",
        "                          sample_fraction: float = 1.0,\n",
        "                          use_spark: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load preprocessed data from MICAP pipeline.\n",
        "\n",
        "        Args:\n",
        "            data_path: Path to processed parquet file or directory\n",
        "            sample_fraction: Fraction of data to use\n",
        "            use_spark: Whether to use Spark for loading (fallback to pandas)\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Loaded and sampled data\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading pipeline data from: {data_path}\")\n",
        "\n",
        "        if use_spark and self.spark and PYSPARK_AVAILABLE:\n",
        "            return self._load_with_spark(data_path, sample_fraction)\n",
        "        else:\n",
        "            return self._load_with_pandas(data_path, sample_fraction)\n",
        "\n",
        "    def _load_with_spark(self, data_path: str, sample_fraction: float) -> pd.DataFrame:\n",
        "        \"\"\"Load data using Spark (maintains original pipeline compatibility).\"\"\"\n",
        "        logger.info(\"Loading data with Spark...\")\n",
        "\n",
        "        try:\n",
        "            # Read parquet file(s)\n",
        "            df = self.spark.read.parquet(data_path)\n",
        "\n",
        "            # Sample if requested\n",
        "            if sample_fraction < 1.0:\n",
        "                df = df.sample(sample_fraction, seed=42)\n",
        "\n",
        "            # Convert to pandas for TensorFlow compatibility\n",
        "            # Use efficient streaming for large datasets\n",
        "            pandas_df = self._spark_to_pandas_efficient(df)\n",
        "\n",
        "            logger.info(f\"Loaded {len(pandas_df)} records with Spark\")\n",
        "            return pandas_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Spark loading failed: {e}\")\n",
        "            logger.info(\"Falling back to pandas...\")\n",
        "            return self._load_with_pandas(data_path, sample_fraction)\n",
        "\n",
        "    def _load_with_pandas(self, data_path: str, sample_fraction: float) -> pd.DataFrame:\n",
        "        \"\"\"Load data with pandas (fallback method).\"\"\"\n",
        "        logger.info(\"Loading data with pandas...\")\n",
        "\n",
        "        try:\n",
        "            # Try reading as parquet first\n",
        "            if data_path.endswith('.parquet') or os.path.isdir(data_path):\n",
        "                df = pd.read_parquet(data_path)\n",
        "            else:\n",
        "                # Fallback to CSV\n",
        "                df = pd.read_csv(data_path)\n",
        "\n",
        "            # Sample if requested\n",
        "            if sample_fraction < 1.0:\n",
        "                df = df.sample(frac=sample_fraction, random_state=42)\n",
        "\n",
        "            logger.info(f\"Loaded {len(df)} records with pandas\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _spark_to_pandas_efficient(self, spark_df, batch_size: int = 50000) -> pd.DataFrame:\n",
        "        \"\"\"Efficiently convert Spark DataFrame to pandas using streaming.\"\"\"\n",
        "        try:\n",
        "            # Try direct conversion for smaller datasets\n",
        "            if spark_df.count() < batch_size:\n",
        "                return spark_df.toPandas()\n",
        "\n",
        "            # Stream large datasets in batches\n",
        "            logger.info(\"Streaming large dataset in batches...\")\n",
        "            parts = []\n",
        "            for batch in spark_df.toLocalIterator(batch_size):\n",
        "                batch_df = pd.DataFrame(list(batch), columns=spark_df.columns)\n",
        "                parts.append(batch_df)\n",
        "\n",
        "            return pd.concat(parts, ignore_index=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Streaming failed, using direct conversion: {e}\")\n",
        "            return spark_df.toPandas()\n",
        "\n",
        "    def validate_pipeline_features(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"\n",
        "        Validate that the DataFrame contains expected pipeline features.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to validate\n",
        "\n",
        "        Returns:\n",
        "            bool: True if valid pipeline data\n",
        "        \"\"\"\n",
        "        # Expected features from MICAP pipeline\n",
        "        required_features = [\n",
        "            'text', 'sentiment', 'text_processed',\n",
        "            'text_length', 'processed_length', 'token_count'\n",
        "        ]\n",
        "\n",
        "        # Optional but expected features\n",
        "        expected_features = [\n",
        "            'vader_compound', 'vader_positive', 'vader_negative', 'vader_neutral',\n",
        "            'emoji_sentiment', 'exclamation_count', 'question_count',\n",
        "            'uppercase_ratio', 'punctuation_density',\n",
        "            'hour_sin', 'hour_cos', 'is_weekend'\n",
        "        ]\n",
        "\n",
        "        # Check required features\n",
        "        missing_required = [f for f in required_features if f not in df.columns]\n",
        "        if missing_required:\n",
        "            logger.error(f\"Missing required pipeline features: {missing_required}\")\n",
        "            return False\n",
        "\n",
        "        # Log available optional features\n",
        "        available_optional = [f for f in expected_features if f in df.columns]\n",
        "        logger.info(f\"Available pipeline features: {len(available_optional)}/{len(expected_features)}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def prepare_features_for_training(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
        "        \"\"\"\n",
        "        Prepare features from pipeline data for deep learning training.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with pipeline features\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (text_sequences, labels, feature_info)\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing pipeline features for training...\")\n",
        "\n",
        "        # Validate data\n",
        "        if not self.validate_pipeline_features(df):\n",
        "            raise ValueError(\"Invalid pipeline data structure\")\n",
        "\n",
        "        # Use processed text for deep learning (already cleaned by pipeline)\n",
        "        text_column = 'text_processed' if 'text_processed' in df.columns else 'text'\n",
        "        texts = df[text_column].fillna('').astype(str)\n",
        "        labels = df['sentiment'].values\n",
        "\n",
        "        # Extract numeric features created by pipeline\n",
        "        numeric_features = []\n",
        "        feature_names = []\n",
        "\n",
        "        # Basic text features\n",
        "        if 'text_length' in df.columns:\n",
        "            numeric_features.append(df['text_length'].fillna(0))\n",
        "            feature_names.append('text_length')\n",
        "\n",
        "        if 'token_count' in df.columns:\n",
        "            numeric_features.append(df['token_count'].fillna(0))\n",
        "            feature_names.append('token_count')\n",
        "\n",
        "        # VADER sentiment features\n",
        "        vader_features = ['vader_compound', 'vader_positive', 'vader_negative', 'vader_neutral']\n",
        "        for feature in vader_features:\n",
        "            if feature in df.columns:\n",
        "                numeric_features.append(df[feature].fillna(0))\n",
        "                feature_names.append(feature)\n",
        "\n",
        "        # Emoji and text statistics\n",
        "        text_stat_features = ['emoji_sentiment', 'exclamation_count', 'question_count',\n",
        "                             'uppercase_ratio', 'punctuation_density']\n",
        "        for feature in text_stat_features:\n",
        "            if feature in df.columns:\n",
        "                numeric_features.append(df[feature].fillna(0))\n",
        "                feature_names.append(feature)\n",
        "\n",
        "        # Temporal features\n",
        "        temporal_features = ['hour_sin', 'hour_cos', 'is_weekend']\n",
        "        for feature in temporal_features:\n",
        "            if feature in df.columns:\n",
        "                numeric_features.append(df[feature].fillna(0))\n",
        "                feature_names.append(feature)\n",
        "\n",
        "        # Combine numeric features\n",
        "        if numeric_features:\n",
        "            numeric_array = np.column_stack(numeric_features)\n",
        "            logger.info(f\"Extracted {len(feature_names)} numeric features: {feature_names}\")\n",
        "        else:\n",
        "            numeric_array = None\n",
        "            logger.warning(\"No numeric features found in pipeline data\")\n",
        "\n",
        "        feature_info = {\n",
        "            'text_column': text_column,\n",
        "            'numeric_features': feature_names,\n",
        "            'numeric_shape': numeric_array.shape if numeric_array is not None else None,\n",
        "            'text_samples': len(texts),\n",
        "            'label_distribution': pd.Series(labels).value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Feature preparation completed: {feature_info}\")\n",
        "\n",
        "        return texts.values, labels, numeric_array, feature_info\n",
        "\n",
        "\n",
        "class GPUEnvironmentManager:\n",
        "    \"\"\"Manages GPU/TPU environment setup and optimization for Google Colab\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device_type = self._detect_accelerator()\n",
        "        self.strategy = self._setup_distribution_strategy()\n",
        "        self.mixed_precision_enabled = False\n",
        "        # self._configure_mixed_precision()\n",
        "        self._log_environment_info()\n",
        "\n",
        "        # if self.device_type == 'GPU':\n",
        "            # self._setup_mixed_precision()\n",
        "\n",
        "    def _detect_accelerator(self) -> str:\n",
        "        \"\"\"Detect available accelerator (GPU/TPU/CPU)\"\"\"\n",
        "        try:\n",
        "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            logger.info(\"TPU detected and initialized\")\n",
        "            return 'TPU'\n",
        "        except (ValueError, RuntimeError):\n",
        "            pass\n",
        "\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            logger.info(f\"GPU detected: {len(gpus)} device(s)\")\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            return 'GPU'\n",
        "\n",
        "        logger.warning(\"No accelerator detected, using CPU\")\n",
        "        return 'CPU'\n",
        "\n",
        "    def _setup_distribution_strategy(self):\n",
        "        \"\"\"Setup distribution strategy based on hardware\"\"\"\n",
        "        if self.device_type == 'TPU':\n",
        "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "            strategy = tf.distribute.TPUStrategy(tpu)\n",
        "            logger.info(f\"Using TPU strategy with {strategy.num_replicas_in_sync} replicas\")\n",
        "        elif self.device_type == 'GPU':\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "            logger.info(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} replicas\")\n",
        "        else:\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "            logger.info(\"Using default strategy (CPU)\")\n",
        "\n",
        "        return strategy\n",
        "\n",
        "    def _setup_mixed_precision(self):\n",
        "        \"\"\"Setup mixed precision training for faster GPU training\"\"\"\n",
        "        try:\n",
        "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "            tf.keras.mixed_precision.set_global_policy(policy)\n",
        "            self.mixed_precision_enabled = True\n",
        "            logger.info(\"Mixed precision training enabled (float16)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not enable mixed precision: {e}\")\n",
        "\n",
        "    def _log_environment_info(self):\n",
        "        \"\"\"Log environment information\"\"\"\n",
        "        logger.info(\"=== GPU/TPU Environment Information ===\")\n",
        "        logger.info(f\"TensorFlow version: {tf.__version__}\")\n",
        "        logger.info(f\"Detected accelerator: {self.device_type}\")\n",
        "\n",
        "        if self.device_type == 'GPU':\n",
        "            gpus = tf.config.list_physical_devices('GPU')\n",
        "            for i, gpu in enumerate(gpus):\n",
        "                logger.info(f\"GPU {i}: {gpu}\")\n",
        "\n",
        "        logger.info(\"=\" * 50)\n",
        "\n",
        "    def get_optimal_batch_size(self, base_batch_size: int = 16384) -> int:\n",
        "        \"\"\"Calculate optimal batch size based on hardware\"\"\"\n",
        "        if self.device_type == 'TPU':\n",
        "            return max(128, base_batch_size * 8)\n",
        "        elif self.device_type == 'GPU':\n",
        "            return base_batch_size * max(1, self.strategy.num_replicas_in_sync)\n",
        "        else:\n",
        "            return max(16, base_batch_size // 2)\n",
        "\n",
        "\n",
        "class PipelineOptimizedModel:\n",
        "    \"\"\"\n",
        "    Base class for pipeline-compatible GPU/TPU optimized deep learning models\n",
        "    Works with features from MICAP preprocessing pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_manager: GPUEnvironmentManager,\n",
        "                 max_words: int = 10000, max_length: int = 100):\n",
        "        self.env_manager = env_manager\n",
        "        self.max_words = max_words\n",
        "        self.max_length = max_length\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.history = None\n",
        "        self.training_time = 0\n",
        "        self.numeric_features_dim = 0\n",
        "\n",
        "    def prepare_data(self, texts: np.ndarray, labels: np.ndarray,\n",
        "                    numeric_features: Optional[np.ndarray] = None,\n",
        "                    validation_split: float = 0.2) -> Tuple:\n",
        "        \"\"\"\n",
        "        Prepare text and numeric features for training.\n",
        "\n",
        "        Args:\n",
        "            texts: Array of text data\n",
        "            labels: Array of labels\n",
        "            numeric_features: Optional array of numeric features from pipeline\n",
        "            validation_split: Validation split ratio\n",
        "\n",
        "        Returns:\n",
        "            Tuple of prepared datasets\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing data for pipeline-compatible training...\")\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        self.tokenizer = Tokenizer(\n",
        "            num_words=self.max_words,\n",
        "            oov_token='<OOV>',\n",
        "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "        )\n",
        "\n",
        "        # Fit tokenizer\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "\n",
        "        # Convert to sequences\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X_text = pad_sequences(sequences, maxlen=self.max_length,\n",
        "                              padding='post', truncating='post')\n",
        "\n",
        "        # Prepare numeric features if available\n",
        "        X_numeric = None\n",
        "        if numeric_features is not None:\n",
        "            X_numeric = numeric_features.astype(np.float32)\n",
        "            self.numeric_features_dim = X_numeric.shape[1]\n",
        "            logger.info(f\"Using {self.numeric_features_dim} numeric features from pipeline\")\n",
        "\n",
        "        y = labels.astype(np.float32)\n",
        "\n",
        "        # Split data\n",
        "        if X_numeric is not None:\n",
        "            X_text_train, X_text_val, X_num_train, X_num_val, y_train, y_val = train_test_split(\n",
        "                X_text, X_numeric, y, test_size=validation_split,\n",
        "                random_state=42, stratify=y\n",
        "            )\n",
        "\n",
        "            return (X_text_train, X_num_train, y_train), (X_text_val, X_num_val, y_val)\n",
        "        else:\n",
        "            X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
        "                X_text, y, test_size=validation_split,\n",
        "                random_state=42, stratify=y\n",
        "            )\n",
        "\n",
        "            return (X_text_train, y_train), (X_text_val, y_val)\n",
        "\n",
        "    def _get_optimizer(self, learning_rate: float = 0.001):\n",
        "        \"\"\"Get optimized optimizer\"\"\"\n",
        "        if self.env_manager.device_type == 'TPU':\n",
        "            optimizer = optimizers.Adam(learning_rate=learning_rate * 2)\n",
        "        else:\n",
        "            optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        if self.env_manager.mixed_precision_enabled:\n",
        "            optimizer = LossScaleOptimizer(optimizer)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def _get_callbacks(self, model_name: str, patience: int = 5):\n",
        "        \"\"\"Get training callbacks\"\"\"\n",
        "        return [\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor='val_loss', patience=patience,\n",
        "                restore_best_weights=True, verbose=1\n",
        "            ),\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss', factor=0.5,\n",
        "                patience=max(2, patience // 2), min_lr=1e-7, verbose=1\n",
        "            ),\n",
        "            callbacks.ModelCheckpoint(\n",
        "                filepath=f'/content/best_{model_name}_pipeline_model.h5',\n",
        "                monitor='val_loss', save_best_only=True, verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "\n",
        "class PipelineLSTMModel(PipelineOptimizedModel):\n",
        "    \"\"\"LSTM model optimized for pipeline features\"\"\"\n",
        "\n",
        "    def build_model(self, embedding_dim: int = 128, lstm_units: int = 64,\n",
        "                   dropout_rate: float = 0.3):\n",
        "        \"\"\"Build LSTM model with optional numeric features integration\"\"\"\n",
        "        logger.info(\"Building pipeline-compatible LSTM model...\")\n",
        "\n",
        "        with self.env_manager.strategy.scope():\n",
        "            # Text input branch\n",
        "            text_input = layers.Input(shape=(self.max_length,), name='text_input')\n",
        "\n",
        "            # Embedding layer\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=self.max_words,\n",
        "                output_dim=embedding_dim,\n",
        "                input_length=self.max_length,\n",
        "                mask_zero=True,\n",
        "                name='embedding'\n",
        "            )(text_input)\n",
        "\n",
        "            embedding = layers.SpatialDropout1D(dropout_rate * 0.5)(embedding)\n",
        "\n",
        "            # Bidirectional LSTM layers\n",
        "            lstm1 = layers.Bidirectional(\n",
        "                layers.LSTM(lstm_units, dropout=dropout_rate,\n",
        "                            return_sequences=True),\n",
        "                name='bi_lstm_1'\n",
        "            )(embedding)\n",
        "\n",
        "            lstm2 = layers.Bidirectional(\n",
        "                layers.LSTM(lstm_units // 2, dropout=dropout_rate,\n",
        "                            return_sequences=False),\n",
        "                name='bi_lstm_2'\n",
        "            )(lstm1)\n",
        "\n",
        "            # Text features\n",
        "            text_features = layers.Dense(64, activation='relu', name='text_dense')(lstm2)\n",
        "            text_features = layers.BatchNormalization()(text_features)\n",
        "            text_features = layers.Dropout(dropout_rate)(text_features)\n",
        "\n",
        "            # Combine with numeric features if available\n",
        "            if self.numeric_features_dim > 0:\n",
        "                # Numeric input branch\n",
        "                numeric_input = layers.Input(shape=(self.numeric_features_dim,), name='numeric_input')\n",
        "                numeric_features = layers.Dense(32, activation='relu', name='numeric_dense')(numeric_input)\n",
        "                numeric_features = layers.BatchNormalization()(numeric_features)\n",
        "                numeric_features = layers.Dropout(dropout_rate * 0.5)(numeric_features)\n",
        "\n",
        "                # Combine text and numeric features\n",
        "                combined = layers.Concatenate(name='combine_features')([text_features, numeric_features])\n",
        "                inputs = [text_input, numeric_input]\n",
        "            else:\n",
        "                combined = text_features\n",
        "                inputs = text_input\n",
        "\n",
        "            # Final classification layers\n",
        "            dense = layers.Dense(32, activation='relu', name='final_dense')(combined)\n",
        "            dense = layers.Dropout(dropout_rate * 0.5)(dense)\n",
        "\n",
        "            output = layers.Dense(1, activation='sigmoid', name='output')(dense)\n",
        "\n",
        "            model = models.Model(inputs=inputs, outputs=output, name='PipelineLSTM')\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        # Compile model\n",
        "        optimizer = self._get_optimizer()\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "        )\n",
        "\n",
        "        logger.info(\"LSTM Model Architecture:\")\n",
        "        self.model.summary(print_fn=logger.info)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, train_data: Tuple, val_data: Tuple, epochs: int = 10):\n",
        "        \"\"\"Train the LSTM model\"\"\"\n",
        "        logger.info(\"Training pipeline LSTM model...\")\n",
        "\n",
        "        callbacks_list = self._get_callbacks('LSTM')\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Prepare training data\n",
        "        if self.numeric_features_dim > 0:\n",
        "            X_text_train, X_num_train, y_train = train_data\n",
        "            X_text_val, X_num_val, y_val = val_data\n",
        "\n",
        "            train_inputs = [X_text_train, X_num_train]\n",
        "            val_inputs = [X_text_val, X_num_val]\n",
        "        else:\n",
        "            X_text_train, y_train = train_data\n",
        "            X_text_val, y_val = val_data\n",
        "\n",
        "            train_inputs = X_text_train\n",
        "            val_inputs = X_text_val\n",
        "\n",
        "        batch_size = self.env_manager.get_optimal_batch_size()\n",
        "\n",
        "        # with self.env_manager.strategy.scope():\n",
        "        #     self.history = self.model.fit(\n",
        "        #         train_inputs, y_train,\n",
        "        #         batch_size=batch_size,\n",
        "        #         epochs=epochs,\n",
        "        #         validation_data=(val_inputs, y_val),\n",
        "        #         callbacks=callbacks_list,\n",
        "        #         verbose=1\n",
        "        #     )\n",
        "        def make_ds(x_text, x_num, y, shuffle=True):\n",
        "            inputs = (x_text, x_num) if x_num is not None else x_text\n",
        "            ds = tf.data.Dataset.from_tensor_slices((inputs, y))\n",
        "            if shuffle:\n",
        "                ds = ds.shuffle(100_000)\n",
        "            return ds.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        train_ds = make_ds(*train_data, shuffle=True)\n",
        "        val_ds   = make_ds(*val_data,   shuffle=False)\n",
        "\n",
        "        with self.env_manager.strategy.scope():\n",
        "            self.history = self.model.fit(\n",
        "                train_ds,\n",
        "                epochs=epochs,\n",
        "                validation_data=val_ds,\n",
        "                callbacks=callbacks_list,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "        self.training_time = time.time() - start_time\n",
        "        logger.info(f\"Training completed in {self.training_time:.2f} seconds\")\n",
        "\n",
        "        return self.history.history\n",
        "\n",
        "\n",
        "def evaluate_pipeline_models_gpu(data_path: str,\n",
        "                                sample_fraction: float = 1.0,\n",
        "                                epochs: int = 10) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate models using MICAP pipeline data with GPU/TPU optimization.\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to processed pipeline data (parquet)\n",
        "        sample_fraction: Fraction of data to use\n",
        "        epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        Dict: Evaluation results\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting pipeline-compatible GPU evaluation...\")\n",
        "\n",
        "    # Initialize components\n",
        "    env_manager = GPUEnvironmentManager()\n",
        "    data_loader = PipelineDataLoader()\n",
        "\n",
        "    # Load pipeline data\n",
        "    df = data_loader.load_pipeline_data(data_path, sample_fraction)\n",
        "\n",
        "    # Prepare features\n",
        "    texts, labels, numeric_features, feature_info = data_loader.prepare_features_for_training(df)\n",
        "\n",
        "    logger.info(f\"Loaded pipeline data: {feature_info}\")\n",
        "\n",
        "    # Initialize models\n",
        "    models_to_evaluate = [\n",
        "        ('Pipeline_LSTM', PipelineLSTMModel)\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model_name, ModelClass in models_to_evaluate:\n",
        "        logger.info(f\"\\n{'='*60}\")\n",
        "        logger.info(f\"Training {model_name}\")\n",
        "        logger.info(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Initialize model\n",
        "            model = ModelClass(env_manager)\n",
        "\n",
        "            # Prepare data\n",
        "            train_data, val_data = model.prepare_data(\n",
        "                texts, labels, numeric_features\n",
        "            )\n",
        "\n",
        "            # Build model\n",
        "            model.build_model()\n",
        "\n",
        "            # Train model\n",
        "            history = model.train(train_data, val_data, epochs=epochs)\n",
        "\n",
        "            # Evaluate\n",
        "            if model.numeric_features_dim > 0:\n",
        "                val_inputs = [val_data[0], val_data[1]]\n",
        "                val_labels = val_data[2]\n",
        "            else:\n",
        "                val_inputs = val_data[0]\n",
        "                val_labels = val_data[1]\n",
        "\n",
        "            eval_results = model.model.evaluate(val_inputs, val_labels, verbose=0)\n",
        "\n",
        "            metrics = {}\n",
        "            for i, metric_name in enumerate(model.model.metrics_names):\n",
        "                metrics[metric_name] = eval_results[i]\n",
        "            metrics['training_time'] = model.training_time\n",
        "\n",
        "            results[model_name] = {\n",
        "                'metrics': metrics,\n",
        "                'history': history,\n",
        "                'feature_info': feature_info\n",
        "            }\n",
        "\n",
        "            # Save model\n",
        "            model.model.save(f'/content/{model_name.lower()}_model.h5')\n",
        "            logger.info(f\"Model saved as {model_name.lower()}_model.h5\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to train {model_name}: {e}\")\n",
        "            results[model_name] = {'error': str(e)}\n",
        "\n",
        "    # Save results\n",
        "    with open('pipeline_model_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "    logger.info(\"Pipeline evaluation completed!\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# Usage examples for Google Colab\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Example usage in Google Colab:\n",
        "\n",
        "    # Option 1: Use your existing pipeline data (RECOMMENDED)\n",
        "    results = evaluate_pipeline_models_gpu(\n",
        "        data_path='/content/pipeline_features.parquet',  # Upload your processed parquet file\n",
        "        sample_fraction=1.0,\n",
        "        epochs=10\n",
        "    )\n",
        "\n",
        "    # Option 2: If you have PySpark setup in Colab (advanced)\n",
        "    # First run your preprocessing pipeline to create the parquet file\n",
        "    # Then use the above approach\n",
        "    \"\"\"\n",
        "\n",
        "    results = evaluate_pipeline_models_gpu(\n",
        "        'part-00000-a1634db0-8c09-4cb9-b1ca-466599ba8394-c000.snappy.parquet',\n",
        "        sample_fraction=1.0,\n",
        "        epochs=5\n",
        "    )\n",
        "    results = evaluate_pipeline_models_gpu(\n",
        "        'part-00001-a1634db0-8c09-4cb9-b1ca-466599ba8394-c000.snappy.parquet',\n",
        "        sample_fraction=1.0,\n",
        "        epochs=5\n",
        "    )\n",
        "    results = evaluate_pipeline_models_gpu(\n",
        "        'part-00002-a1634db0-8c09-4cb9-b1ca-466599ba8394-c000.snappy.parquet',\n",
        "        sample_fraction=1.0,\n",
        "        epochs=5\n",
        "    )\n",
        "    results = evaluate_pipeline_models_gpu(\n",
        "        'part-00003-a1634db0-8c09-4cb9-b1ca-466599ba8394-c000.snappy.parquet',\n",
        "        sample_fraction=1.0,\n",
        "        epochs=5\n",
        "    )\n",
        "\n",
        "    logger.info(\"Pipeline-compatible GPU deep learning models ready!\")\n",
        "    logger.info(\"Upload your 'pipeline_features.parquet' file to /content/ and run evaluation.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}