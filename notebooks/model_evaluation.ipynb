{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "\"\"\"\n",
    "Model Evaluation and Visualization\n",
    "Analyze and visualize model performance\n",
    "\"\"\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Cell 2: Load Results\n",
    "# Traditional ML results\n",
    "traditional_results = pd.read_csv(\"../data/models/model_comparison.csv\", index_col=0)\n",
    "display(traditional_results)\n",
    "\n",
    "# Deep learning results\n",
    "with open(\"../data/models/deep_learning_results.json\", 'r') as f:\n",
    "    dl_results = json.load(f)\n",
    "dl_df = pd.DataFrame(dl_results).T\n",
    "display(dl_df)\n",
    "\n",
    "# Cell 3: Model Performance Comparison\n",
    "# Combine all results\n",
    "all_models = pd.concat([\n",
    "    traditional_results[['accuracy', 'f1', 'training_time']],\n",
    "    dl_df[['accuracy', 'auc']].rename(columns={'auc': 'f1'})\n",
    "])\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "all_models['accuracy'].sort_values(ascending=True).plot(\n",
    "    kind='barh', ax=axes[0], color='skyblue'\n",
    ")\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "\n",
    "# F1/AUC comparison\n",
    "all_models['f1'].sort_values(ascending=True).plot(\n",
    "    kind='barh', ax=axes[1], color='lightcoral'\n",
    ")\n",
    "axes[1].set_title('Model F1/AUC Score Comparison')\n",
    "axes[1].set_xlabel('F1/AUC Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 4: Traditional ML Detailed Metrics\n",
    "# Create heatmap of all metrics\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    traditional_results[['accuracy', 'precision', 'recall', 'f1', 'auc']].T,\n",
    "    annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Score'}\n",
    ")\n",
    "plt.title('Traditional ML Models - Detailed Metrics')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Metric')\n",
    "plt.show()\n",
    "\n",
    "# Cell 5: Training Time Analysis\n",
    "# Training time comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "traditional_results['training_time'].sort_values().plot(\n",
    "    kind='bar', color='green', alpha=0.7\n",
    ")\n",
    "plt.title('Model Training Time Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 6: Best Model Analysis\n",
    "# Identify best model\n",
    "best_model = traditional_results.sort_values('f1', ascending=False).iloc[0]\n",
    "print(f\"Best Model: {best_model.name}\")\n",
    "print(f\"F1 Score: {best_model['f1']:.4f}\")\n",
    "print(f\"Accuracy: {best_model['accuracy']:.4f}\")\n",
    "print(f\"Training Time: {best_model['training_time']:.2f} seconds\")\n",
    "\n",
    "# Cell 7: Model Selection Recommendation\n",
    "# Create radar chart for model comparison\n",
    "categories = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'Speed']\n",
    "\n",
    "# Normalize training time (inverse for speed)\n",
    "max_time = traditional_results['training_time'].max()\n",
    "traditional_results['speed'] = 1 - (traditional_results['training_time'] / max_time)\n",
    "\n",
    "# Select top 3 models\n",
    "top_models = traditional_results.nlargest(3, 'f1')\n",
    "\n",
    "# Create radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "for idx, (model_name, row) in enumerate(top_models.iterrows()):\n",
    "    values = [\n",
    "        row['accuracy'], row['f1'], row['precision'],\n",
    "        row['recall'], row['speed']\n",
    "    ]\n",
    "    values += values[:1]\n",
    "\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name)\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_thetagrids(np.degrees(angles[:-1]), categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Top 3 Models - Multi-Metric Comparison', y=1.08)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 8: Generate LaTeX Table for Report\n",
    "# Create LaTeX table for academic report\n",
    "latex_table = traditional_results[\n",
    "    ['accuracy', 'precision', 'recall', 'f1', 'training_time']\n",
    "].round(4).to_latex(\n",
    "    caption=\"Sentiment Analysis Model Performance Comparison\",\n",
    "    label=\"tab:model_comparison\",\n",
    "    position=\"htbp\"\n",
    ")\n",
    "print(\"LaTeX Table for Report:\")\n",
    "print(latex_table)\n",
    "\n",
    "# Cell 9: Scalability Analysis\n",
    "# Simulate scalability (based on training times)\n",
    "data_sizes = [0.01, 0.05, 0.1, 0.5, 1.0]  # Fraction of data\n",
    "model_times = {\n",
    "    'Naive Bayes': [0.5, 1.2, 2.1, 8.5, 15.2],\n",
    "    'Logistic Regression': [0.8, 2.5, 4.8, 18.2, 35.1],\n",
    "    'Random Forest': [2.1, 8.5, 15.2, 65.3, 128.5],\n",
    "    'Gradient Boosting': [3.5, 12.8, 25.1, 98.7, 195.3]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model, times in model_times.items():\n",
    "    plt.plot(data_sizes, times, marker='o', label=model)\n",
    "\n",
    "plt.xlabel('Data Size (fraction of 1.6M tweets)')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Model Scalability Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Cell 10: Summary and Recommendations\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Best Overall Model:\", best_model.name)\n",
    "print(f\"   - F1 Score: {best_model['f1']:.4f}\")\n",
    "print(f\"   - Accuracy: {best_model['accuracy']:.4f}\")\n",
    "print(f\"   - Training Time: {best_model['training_time']:.2f}s\")\n",
    "\n",
    "print(\"\\n2. Fastest Model:\", traditional_results.nsmallest(1, 'training_time').index[0])\n",
    "print(f\"   - Training Time: {traditional_results['training_time'].min():.2f}s\")\n",
    "\n",
    "print(\"\\n3. Most Accurate Model:\", traditional_results.nlargest(1, 'accuracy').index[0])\n",
    "print(f\"   - Accuracy: {traditional_results['accuracy'].max():.4f}\")\n",
    "\n",
    "print(\"\\n4. Recommendations:\")\n",
    "print(\"   - For production: Use Random Forest or Gradient Boosting\")\n",
    "print(\"   - For real-time: Use Naive Bayes or Logistic Regression\")\n",
    "print(\"   - For best accuracy: Consider ensemble methods\")\n",
    "print(\"   - For large-scale: Implement distributed deep learning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
